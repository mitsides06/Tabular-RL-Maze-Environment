# -*- coding: utf-8 -*-
"""CW_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aQDoLE1-hXDZTwEM153V5qKSOqr4czMv
"""

import numpy as np
import random
import matplotlib.pyplot as plt # Graphical library
#from sklearn.metrics import mean_squared_error # Mean-squared error function

"""# Coursework 1 :
See pdf for instructions.
"""

# WARNING: fill in these two functions that will be used by the auto-marking script
# [Action required]

def get_CID():
  return "01857560" # Return your CID (add 0 at the beginning to ensure it is 8 digits long)

def get_login():
  return "km2120" # Return your short imperial login

"""## Helper class"""

# This class is used ONLY for graphics
# YOU DO NOT NEED to understand it to work on this coursework

class GraphicsMaze(object):

  def __init__(self, shape, locations, default_reward, obstacle_locs, absorbing_locs, absorbing_rewards, absorbing):

    self.shape = shape
    self.locations = locations
    self.absorbing = absorbing

    # Walls
    self.walls = np.zeros(self.shape)
    for ob in obstacle_locs:
      self.walls[ob] = 20

    # Rewards
    self.rewarders = np.ones(self.shape) * default_reward
    for i, rew in enumerate(absorbing_locs):
      self.rewarders[rew] = 10 if absorbing_rewards[i] > 0 else -10

    # Print the map to show it
    self.paint_maps()

  def paint_maps(self):
    """
    Print the Maze topology (obstacles, absorbing states and rewards)
    input: /
    output: /
    """
    plt.figure(figsize=(15,10))
    plt.imshow(self.walls + self.rewarders)
    plt.show()

  def paint_state(self, state):
    """
    Print one state on the Maze topology (obstacles, absorbing states and rewards)
    input: /
    output: /
    """
    states = np.zeros(self.shape)
    states[state] = 30
    plt.figure(figsize=(15,10))
    plt.imshow(self.walls + self.rewarders + states)
    plt.show()

  def draw_deterministic_policy(self, Policy):
    """
    Draw a deterministic policy
    input: Policy {np.array} -- policy to draw (should be an array of values between 0 and 3 (actions))
    output: /
    """
    plt.figure(figsize=(15,10))
    plt.imshow(self.walls + self.rewarders) # Create the graph of the Maze
    for state, action in enumerate(Policy):
      if(self.absorbing[0,state]): # If it is an absorbing state, don't plot any action
        continue
      arrows = [r"$\uparrow$",r"$\rightarrow$", r"$\downarrow$", r"$\leftarrow$"] # List of arrows corresponding to each possible action
      action_arrow = arrows[action] # Take the corresponding action
      location = self.locations[state] # Compute its location on graph
      plt.text(location[1], location[0], action_arrow, ha='center', va='center') # Place it on graph
    plt.savefig("MC_policy.png")
    plt.show()

  def draw_policy(self, Policy):
    """
    Draw a policy (draw an arrow in the most probable direction)
    input: Policy {np.array} -- policy to draw as probability
    output: /
    """
    deterministic_policy = np.array([np.argmax(Policy[row,:]) for row in range(Policy.shape[0])])
    self.draw_deterministic_policy(deterministic_policy)

  def draw_value(self, Value):
    """
    Draw a policy value
    input: Value {np.array} -- policy values to draw
    output: /
    """
    plt.figure(figsize=(15,10))
    plt.imshow(self.walls + self.rewarders) # Create the graph of the Maze
    for state, value in enumerate(Value):
      if(self.absorbing[0, state]): # If it is an absorbing state, don't plot any value
        continue
      location = self.locations[state] # Compute the value location on graph
      plt.text(location[1], location[0], round(value,2), ha='center', va='center') # Place it on graph
    plt.savefig("MC_value.png")
    plt.show()

  def draw_deterministic_policy_grid(self, Policies, title, n_columns, n_lines):
    """
    Draw a grid representing multiple deterministic policies
    input: Policies {np.array of np.array} -- array of policies to draw (each should be an array of values between 0 and 3 (actions))
    output: /
    """
    plt.figure(figsize=(20,8))
    for subplot in range (len(Policies)): # Go through all policies
      ax = plt.subplot(n_columns, n_lines, subplot+1) # Create a subplot for each policy
      ax.imshow(self.walls+self.rewarders) # Create the graph of the Maze
      for state, action in enumerate(Policies[subplot]):
        if(self.absorbing[0,state]): # If it is an absorbing state, don't plot any action
          continue
        arrows = [r"$\uparrow$",r"$\rightarrow$", r"$\downarrow$", r"$\leftarrow$"] # List of arrows corresponding to each possible action
        action_arrow = arrows[action] # Take the corresponding action
        location = self.locations[state] # Compute its location on graph
        plt.text(location[1], location[0], action_arrow, ha='center', va='center') # Place it on graph
      ax.title.set_text(title[subplot]) # Set the title for the graph given as argument
    plt.show()

  def draw_policy_grid(self, Policies, title, n_columns, n_lines):
    """
    Draw a grid representing multiple policies (draw an arrow in the most probable direction)
    input: Policy {np.array} -- array of policies to draw as probability
    output: /
    """
    deterministic_policies = np.array([[np.argmax(Policy[row,:]) for row in range(Policy.shape[0])] for Policy in Policies])
    self.draw_deterministic_policy_grid(deterministic_policies, title, n_columns, n_lines)

  def draw_value_grid(self, Values, title, n_columns, n_lines):
    """
    Draw a grid representing multiple policy values
    input: Values {np.array of np.array} -- array of policy values to draw
    output: /
    """
    plt.figure(figsize=(20,8))
    for subplot in range (len(Values)): # Go through all values
      ax = plt.subplot(n_columns, n_lines, subplot+1) # Create a subplot for each value
      ax.imshow(self.walls+self.rewarders) # Create the graph of the Maze
      for state, value in enumerate(Values[subplot]):
        if(self.absorbing[0,state]): # If it is an absorbing state, don't plot any value
          continue
        location = self.locations[state] # Compute the value location on graph
        plt.text(location[1], location[0], round(value,1), ha='center', va='center') # Place it on graph
      ax.title.set_text(title[subplot]) # Set the title for the graoh given as argument
    plt.show()

"""## Maze class"""

# This class define the Maze environment

class Maze(object):

  # [Action required]
  def __init__(self):
    """
    Maze initialisation.
    input: /
    output: /
    """

    # [Action required]
    # Properties set from the CID
    self._prob_success = 0.8 + 0.02 * (9-6) # float
    self._gamma = 0.8 + 0.02 * 6 # float
    self._goal = 0 # integer (0 for R0, 1 for R1, 2 for R2, 3 for R3)

    # Build the maze
    self._build_maze()


  # Functions used to build the Maze environment
  # You DO NOT NEED to modify them
  def _build_maze(self):
    """
    Maze initialisation.
    input: /
    output: /
    """

    # Properties of the maze
    self._shape = (13, 10)
    self._obstacle_locs = [
                          (1,0), (1,1), (1,2), (1,3), (1,4), (1,7), (1,8), (1,9), \
                          (2,1), (2,2), (2,3), (2,7), \
                          (3,1), (3,2), (3,3), (3,7), \
                          (4,1), (4,7), \
                          (5,1), (5,7), \
                          (6,5), (6,6), (6,7), \
                          (8,0), \
                          (9,0), (9,1), (9,2), (9,6), (9,7), (9,8), (9,9), \
                          (10,0)
                         ] # Location of obstacles
    self._absorbing_locs = [(2,0), (2,9), (10,1), (12,9)] # Location of absorbing states
    self._absorbing_rewards = [ (500 if (i == self._goal) else -50) for i in range (4) ]
    self._starting_locs = [(0,0), (0,1), (0,2), (0,3), (0,4), (0,5), (0,6), (0,7), (0,8), (0,9)] #Reward of absorbing states
    self._default_reward = -1 # Reward for each action performs in the environment
    self._max_t = 500 # Max number of steps in the environment

    # Actions
    self._action_size = 4
    self._direction_names = ['N','E','S','W'] # Direction 0 is 'N', 1 is 'E' and so on

    # States
    self._locations = []
    for i in range (self._shape[0]):
      for j in range (self._shape[1]):
        loc = (i,j)
        # Adding the state to locations if it is no obstacle
        if self._is_location(loc):
          self._locations.append(loc)
    self._state_size = len(self._locations)

    # Neighbours - each line is a state, ranked by state-number, each column is a direction (N, E, S, W)
    self._neighbours = np.zeros((self._state_size, 4))

    for state in range(self._state_size):
      loc = self._get_loc_from_state(state)

      # North
      neighbour = (loc[0]-1, loc[1]) # North neighbours location
      if self._is_location(neighbour):
        self._neighbours[state][self._direction_names.index('N')] = self._get_state_from_loc(neighbour)
      else: # If there is no neighbour in this direction, coming back to current state
        self._neighbours[state][self._direction_names.index('N')] = state

      # East
      neighbour = (loc[0], loc[1]+1) # East neighbours location
      if self._is_location(neighbour):
        self._neighbours[state][self._direction_names.index('E')] = self._get_state_from_loc(neighbour)
      else: # If there is no neighbour in this direction, coming back to current state
        self._neighbours[state][self._direction_names.index('E')] = state

      # South
      neighbour = (loc[0]+1, loc[1]) # South neighbours location
      if self._is_location(neighbour):
        self._neighbours[state][self._direction_names.index('S')] = self._get_state_from_loc(neighbour)
      else: # If there is no neighbour in this direction, coming back to current state
        self._neighbours[state][self._direction_names.index('S')] = state

      # West
      neighbour = (loc[0], loc[1]-1) # West neighbours location
      if self._is_location(neighbour):
        self._neighbours[state][self._direction_names.index('W')] = self._get_state_from_loc(neighbour)
      else: # If there is no neighbour in this direction, coming back to current state
        self._neighbours[state][self._direction_names.index('W')] = state

    # Absorbing
    self._absorbing = np.zeros((1, self._state_size))
    for a in self._absorbing_locs:
      absorbing_state = self._get_state_from_loc(a)
      self._absorbing[0, absorbing_state] = 1

    # Transition matrix
    self._T = np.zeros((self._state_size, self._state_size, self._action_size)) # Empty matrix of domension S*S*A
    for action in range(self._action_size):
      for outcome in range(4): # For each direction (N, E, S, W)
        # The agent has prob_success probability to go in the correct direction
        if action == outcome:
          prob = 1 - 3.0 * ((1.0 - self._prob_success) / 3.0) # (theoritically equal to self.prob_success but avoid rounding error and garanty a sum of 1)
        # Equal probability to go into one of the other directions
        else:
          prob = (1.0 - self._prob_success) / 3.0

        # Write this probability in the transition matrix
        for prior_state in range(self._state_size):
          # If absorbing state, probability of 0 to go to any other states
          if not self._absorbing[0, prior_state]:
            post_state = self._neighbours[prior_state, outcome] # Post state number
            post_state = int(post_state) # Transform in integer to avoid error
            self._T[prior_state, post_state, action] += prob

    # Reward matrix
    self._R = np.ones((self._state_size, self._state_size, self._action_size)) # Matrix filled with 1
    self._R = self._default_reward * self._R # Set default_reward everywhere
    for i in range(len(self._absorbing_rewards)): # Set absorbing states rewards
      post_state = self._get_state_from_loc(self._absorbing_locs[i])
      self._R[:,post_state,:] = self._absorbing_rewards[i]

    # Creating the graphical Maze world
    self._graphics = GraphicsMaze(self._shape, self._locations, self._default_reward, self._obstacle_locs, self._absorbing_locs, self._absorbing_rewards, self._absorbing)

    # Reset the environment
    self.reset()


  def _is_location(self, loc):
    """
    Is the location a valid state (not out of Maze and not an obstacle)
    input: loc {tuple} -- location of the state
    output: _ {bool} -- is the location a valid state
    """
    if (loc[0] < 0 or loc[1] < 0 or loc[0] > self._shape[0]-1 or loc[1] > self._shape[1]-1):
      return False
    elif (loc in self._obstacle_locs):
      return False
    else:
      return True


  def _get_state_from_loc(self, loc):
    """
    Get the state number corresponding to a given location
    input: loc {tuple} -- location of the state
    output: index {int} -- corresponding state number
    """
    return self._locations.index(tuple(loc))


  def _get_loc_from_state(self, state):
    """
    Get the state number corresponding to a given location
    input: index {int} -- state number
    output: loc {tuple} -- corresponding location
    """
    return self._locations[state]

  # Getter functions used only for DP agents
  # You DO NOT NEED to modify them
  def get_T(self):
    return self._T

  def get_R(self):
    return self._R

  def get_absorbing(self):
    return self._absorbing

  # Getter functions used for DP, MC and TD agents
  # You DO NOT NEED to modify them
  def get_graphics(self):
    return self._graphics

  def get_action_size(self):
    return self._action_size

  def get_state_size(self):
    return self._state_size

  def get_gamma(self):
    return self._gamma

  # Functions used to perform episodes in the Maze environment
  def reset(self):
    """
    Reset the environment state to one of the possible starting states
    input: /
    output:
      - t {int} -- current timestep
      - state {int} -- current state of the envionment
      - reward {int} -- current reward
      - done {bool} -- True if reach a terminal state / 0 otherwise
    """
    self._t = 0
    self._state = self._get_state_from_loc(self._starting_locs[random.randrange(len(self._starting_locs))])
    self._reward = 0
    self._done = False
    return self._t, self._state, self._reward, self._done

  def step(self, action):
    """
    Perform an action in the environment
    input: action {int} -- action to perform
    output:
      - t {int} -- current timestep
      - state {int} -- current state of the envionment
      - reward {int} -- current reward
      - done {bool} -- True if reach a terminal state / 0 otherwise
    """

    # If environment already finished, print an error
    if self._done or self._absorbing[0, self._state]:
      print("Please reset the environment")
      return self._t, self._state, self._reward, self._done

    # Drawing a random number used for probaility of next state
    probability_success = random.uniform(0,1)

    # Look for the first possible next states (so get a reachable state even if probability_success = 0)
    new_state = 0
    while self._T[self._state, new_state, action] == 0:
      new_state += 1
    assert self._T[self._state, new_state, action] != 0, "Selected initial state should be probability 0, something might be wrong in the environment."

    # Find the first state for which probability of occurence matches the random value
    total_probability = self._T[self._state, new_state, action]
    while (total_probability < probability_success) and (new_state < self._state_size-1):
     new_state += 1
     total_probability += self._T[self._state, new_state, action]
    assert self._T[self._state, new_state, action] != 0, "Selected state should be probability 0, something might be wrong in the environment."

    # Setting new t, state, reward and done
    self._t += 1
    self._reward = self._R[self._state, new_state, action]
    self._done = self._absorbing[0, new_state] or self._t > self._max_t
    self._state = new_state
    return self._t, self._state, self._reward, self._done

"""## DP Agent"""

# This class define the Dynamic Programing agent

class DP_agent(object):

  # [Action required]
  # WARNING: make sure this function can be called by the auto-marking script

  def solve(self, env):
    """
    Solve a given Maze environment using Dynamic Programming
    input: env {Maze object} -- Maze to solve
    output:
      - policy {np.array} -- Optimal policy found to solve the given Maze environment
      - V {np.array} -- Corresponding value function
    """

    # Initialisation (can be edited)
    policy = np.zeros((env.get_state_size(), env.get_action_size()))
    policy += 0.25
    V = np.zeros(env.get_state_size())
    theta = 0.0001
    gamma = env.get_gamma()
    delta = 2 * theta
    trans_matrix = env.get_T()
    rewards_matrix = env.get_R()

    return self.value_iteration(policy, V, theta, gamma, rewards_matrix, trans_matrix, env, delta)

  def value_iteration(self, policy, V, theta, gamma, rewards_matrix, trans_matrix, env, delta):
    while delta >= theta:
      delta = 0
      copied_V = V.copy()
      V_matrix = np.array([list(copied_V)] * 4)
      V_matrix = V_matrix.T

      for state in range(env.get_state_size()):
        v = V[state]
        reward = rewards_matrix[state]
        transition = trans_matrix[state]
        reward_value_sum = reward + gamma * V_matrix
        result_matrix = np.multiply(transition, reward_value_sum)
        V[state] = np.max(sum(result_matrix))
        delta = max(delta, abs(v - V[state]))

    copied_V = V.copy()
    V_matrix = np.array([list(copied_V)] * 4)
    V_matrix = V_matrix.T

    for state in range(env.get_state_size()):
      reward = rewards_matrix[state]
      transition = trans_matrix[state]
      reward_value_sum = reward + gamma * V_matrix
      result_matrix = np.multiply(transition, reward_value_sum)
      optimal_action = np.argmax(sum(result_matrix))
      policy[state] = np.zeros(policy.shape[1])
      policy[state][optimal_action] = 1

    return policy, V

"""## MC agent"""

# This class define the Monte-Carlo agent

class MC_agent(object):

  # [Action required]
  def create_episode(self, env, policy):
    curr_step = env.reset()
    curr_state = curr_step[1]
    stop = curr_step[-1]
    episode = []
    while not stop:
      action = np.random.choice(range(env.get_action_size()), p=policy[curr_state])
      next_step = env.step(action)
      curr_reward = next_step[2]
      episode.append([(curr_state, action), curr_reward])
      curr_state = next_step[1]
      stop = next_step[-1]

    total_reward = sum([i[-1] for i in episode])

    return episode, total_reward

  def update(self, env, episode, gamma, Q, policy, epsilon, values, alpha):
    reversed_episode = list(reversed(episode))
    G = 0
    for idx,step in enumerate(reversed_episode):
      G = gamma * G + step[-1]
      if step[0] not in [i[0] for i in reversed_episode[idx+1:]]:
        state = step[0][0]
        action = step[0][1]
        Q[state, action] = Q[state, action] + alpha * (G - Q[state, action])
        optimal_action = np.argmax(Q[state])
        for action in range(env.get_action_size()):
          if action == optimal_action:
            policy[state, action] = 1 - epsilon + (epsilon / env.get_action_size())
          else:
            policy[state, action] = epsilon / env.get_action_size()

    new_V = np.sum(policy * Q, axis=1)
    values.append(new_V)


  # WARNING: make sure this function can be called by the auto-marking script
  def solve(self, env):
    """
    Solve a given Maze environment using Monte Carlo learning
    input: env {Maze object} -- Maze to solve
    output:
      - policy {np.array} -- Optimal policy found to solve the given Maze environment
      - values {list of np.array} -- List of successive value functions for each episode
      - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
    """
    Q = np.random.rand(env.get_state_size(), env.get_action_size())
    V = np.zeros(env.get_state_size())
    policy = np.zeros((env.get_state_size(), env.get_action_size()))
    policy += 0.25
    values = [V]
    total_rewards = []
    gamma = env.get_gamma()
    epsilon = 1
    alpha = 0.05

    iter = 0
    while iter < 10000:
      episode, reward = self.create_episode(env, policy)
      self.update(env, episode, gamma, Q, policy, epsilon, values, alpha)
      total_rewards.append(reward)
      if iter > 1000:
        if reward > 0:
          epsilon *= 0.999

      iter += 1

    return policy, values, total_rewards

"""## TD agent"""

# This class define the Temporal-Difference agent

class TD_agent(object):

  # [Action required]
  # WARNING: make sure this function can be called by the auto-marking script

  def create_episode(self, env, epsilon, Q, alpha, gamma):
    curr_step = env.reset()
    curr_state = curr_step[1]
    stop = curr_step[-1]
    total_reward = 0
    while not stop:
      probs = self.epsilon_greedy(env, curr_state, Q, epsilon)
      action = np.random.choice(range(env.get_action_size()), p=probs)
      next_step = env.step(action)
      stop = next_step[-1]
      curr_reward = next_step[2]
      total_reward += curr_reward
      next_state = next_step[1]
      self.update(Q, action, curr_state, next_state, curr_reward, alpha, gamma)
      curr_state = next_state

    return total_reward

  def convert_Q_to_policy(self,Q, policy):
    idxs = np.argmax(Q, axis=1)
    for state, idx in enumerate(idxs):
      policy[state][idx] = 1
    return policy




  def epsilon_greedy(self, env, curr_state, Q, epsilon):
    probs = [0] * env.get_action_size()
    optimal_action = np.argmax(Q[curr_state])
    for action in range(env.get_action_size()):
      if action == optimal_action:
        probs[action] = 1 - epsilon + (epsilon / env.get_action_size())
      else:
        probs[action]  = epsilon / env.get_action_size()

    return probs

  def update(self, Q, action, curr_state, next_state, curr_reward, alpha, gamma):
    optimal_next_Q = np.max(Q[next_state])
    curr_Q = Q[curr_state, action]
    Q[curr_state, action] = curr_Q + alpha * (curr_reward + gamma * optimal_next_Q - curr_Q)



  def solve(self, env):
    """
    Solve a given Maze environment using Temporal Difference learning
    input: env {Maze object} -- Maze to solve
    output:
      - policy {np.array} -- Optimal policy found to solve the given Maze environment
      - values {list of np.array} -- List of successive value functions for each episode
      - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
    """

    Q = np.random.rand(env.get_state_size(), env.get_action_size())
    V = np.zeros(env.get_state_size())
    policy = np.zeros((env.get_state_size(), env.get_action_size()))
    values = [V]
    total_rewards = []
    epsilon = 1
    gamma = env.get_gamma()
    alpha = 0.05

    ####
    # Add your code here
    # WARNING: this agent only has access to env.reset() and env.step()
    # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value
    ####
    iter = 0
    while iter < 10000:

      total_reward = self.create_episode(env, epsilon, Q, alpha, gamma)
      new_V = np.max(Q, axis=1)
      values.append(new_V)

      total_rewards.append(total_reward)

      if iter > 4000:
        if total_reward > 0:
          epsilon *= 0.99
      elif iter > 3750:
        epsilon = 0.3
      elif iter > 3500:
        epsilon = 0.5
      elif iter > 3250:
        epsilon = 0.7
      elif iter > 3000:
        epsilon = 0.75
      elif iter > 2500:
        epsilon = 0.8
      elif iter > 2000:
        epsilon = 0.85
      elif iter > 1500:
        epsilon = 0.9
      elif iter > 1000:
        epsilon = 0.95
      iter += 1

    policy = self.convert_Q_to_policy(Q, policy)

    return policy, values, total_rewards

"""## Example main"""

# Example main (can be edited)

### Question 0: Defining the environment

print("Creating the Maze:\n")
maze = Maze()


### Question 1: Dynamic programming

dp_agent = DP_agent()
dp_policy, dp_value = dp_agent.solve(maze)

print("Results of the DP agent:\n")
maze.get_graphics().draw_policy(dp_policy)
maze.get_graphics().draw_value(dp_value)

### Question 2: Monte-Carlo learning

mc_agent = MC_agent()
mc_policy, mc_values, total_rewards = mc_agent.solve(maze)

print("Results of the MC agent:\n")
maze.get_graphics().draw_policy(mc_policy)
maze.get_graphics().draw_value(mc_values[-1])


### Question 3: Temporal-Difference learning

td_agent = TD_agent()
td_policy, td_values, total_rewards = td_agent.solve(maze)

print("Results of the TD agent:\n")
maze.get_graphics().draw_policy(td_policy)
maze.get_graphics().draw_value(td_values[-1])

"""# MC Plots"""

if __name__ == "__main__":
  def test_1():

    maze = Maze()


    means = np.zeros((25, 10000))
    stds = np.zeros((25, 10000))

    for i in range(25):
      mc_agent = MC_agent()
      _, _, total_rewards = mc_agent.solve(maze)
      means[i] = total_rewards
      stds[i] = total_rewards
      print(i)

    means = np.mean(means, axis=0)
    stds = np.std(stds, axis=0)
    episodes = np.arange(1,10001)

    plt.figure()
    plt.plot(episodes, means, "b")
    plt.plot(episodes, stds, "r")
    plt.legend(["mean", "std"], loc="lower right")
    plt.title("mean vs std")
    plt.xlabel("number of episodes")
    plt.ylabel("total non-discounted sum of rewards")
    plt.savefig("q2_part3_new.png")
    plt.show()

"""# TD Plots"""
if __name__ =="__main__":
  def test_2():

    class TD_agent(object):

      # [Action required]
      # WARNING: make sure this function can be called by the auto-marking script

      def create_episode(self, env, epsilon, Q, alpha, gamma):
        curr_step = env.reset()
        curr_state = curr_step[1]
        stop = curr_step[-1]
        total_reward = 0
        while not stop:
          probs = self.epsilon_greedy(env, curr_state, Q, epsilon)
          action = np.random.choice(range(env.get_action_size()), p=probs)
          next_step = env.step(action)
          stop = next_step[-1]
          curr_reward = next_step[2]
          total_reward += curr_reward
          next_state = next_step[1]
          self.update(Q, action, curr_state, next_state, curr_reward, alpha, gamma)
          curr_state = next_state

        return total_reward

      def convert_Q_to_policy(self,Q, policy):
        idxs = np.argmax(Q, axis=1)
        for state, idx in enumerate(idxs):
          policy[state][idx] = 1
        return policy




      def epsilon_greedy(self, env, curr_state, Q, epsilon):
        probs = [0] * env.get_action_size()
        optimal_action = np.argmax(Q[curr_state])
        for action in range(env.get_action_size()):
          if action == optimal_action:
            probs[action] = 1 - epsilon + (epsilon / env.get_action_size())
          else:
            probs[action]  = epsilon / env.get_action_size()

        return probs

      def update(self, Q, action, curr_state, next_state, curr_reward, alpha, gamma):
        optimal_next_Q = np.max(Q[next_state])
        curr_Q = Q[curr_state, action]
        #alpha = 1 / (iter)**2
        Q[curr_state, action] = curr_Q + alpha * (curr_reward + gamma * optimal_next_Q - curr_Q)



      def solve(self, env, alpha):
        """
        Solve a given Maze environment using Temporal Difference learning
        input: env {Maze object} -- Maze to solve
        output:
          - policy {np.array} -- Optimal policy found to solve the given Maze environment
          - values {list of np.array} -- List of successive value functions for each episode
          - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
        """

        Q = np.random.rand(env.get_state_size(), env.get_action_size())
        V = np.zeros(env.get_state_size())
        policy = np.zeros((env.get_state_size(), env.get_action_size()))
        values = [V]
        total_rewards = []
        epsilon = 0.1
        gamma = env.get_gamma()

        ####
        # Add your code here
        # WARNING: this agent only has access to env.reset() and env.step()
        # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value
        ####
        iter = 0
        while iter < 10000:

          total_reward = self.create_episode(env, epsilon, Q, alpha, gamma)
          new_V = np.max(Q, axis=1)
          values.append(new_V)

          total_rewards.append(total_reward)

          iter += 1

        policy = self.convert_Q_to_policy(Q, policy)

        return policy, values, total_rewards

      def solve_early(self, env, alpha):
        """
        Solve a given Maze environment using Temporal Difference learning
        input: env {Maze object} -- Maze to solve
        output:
          - policy {np.array} -- Optimal policy found to solve the given Maze environment
          - values {list of np.array} -- List of successive value functions for each episode
          - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
        """

        Q = np.random.rand(env.get_state_size(), env.get_action_size())
        V = np.zeros(env.get_state_size())
        policy = np.zeros((env.get_state_size(), env.get_action_size()))
        values = [V]
        total_rewards = []
        epsilon = 0.1
        gamma = env.get_gamma()

        ####
        # Add your code here
        # WARNING: this agent only has access to env.reset() and env.step()
        # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value
        ####
        iter = 0
        while iter < 100:

          total_reward = self.create_episode(env, epsilon, Q, alpha, gamma)
          new_V = np.max(Q, axis=1)
          values.append(new_V)

          total_rewards.append(total_reward)

          iter += 1

        policy = self.convert_Q_to_policy(Q, policy)

        return policy, values, total_rewards

    maze = Maze()
    alphas_dict = {}

    alphas = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]
    for alpha in alphas:



      means = np.zeros((5, 10000))
      stds = np.zeros((5, 10000))

      for i in range(5):
        td_agent = TD_agent()
        _, _, total_rewards = td_agent.solve(maze, alpha)
        means[i] = total_rewards
        stds[i] = total_rewards

      means = np.mean(means, axis=0)
      stds = np.std(stds, axis=0)
      episodes = np.arange(1,10001)
      alphas_dict[alpha] = [means, stds]

      plt.figure()
      plt.plot(episodes, means, "b")
      plt.plot(episodes, stds, "r")
      plt.legend(["mean", "std"], loc="lower right")
      plt.title("mean vs std")
      plt.xlabel("number of episodes")
      plt.ylabel("discounted total reward")
      plt.savefig(f"learning_curve_of_alpha={alpha}.png")
      plt.show()

    maze = Maze()
    alphas_early_dict = {}

    alphas = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]
    for alpha in alphas:



      means = np.zeros((500, 100))
      stds = np.zeros((500, 100))

      for i in range(500):
        td_agent = TD_agent()
        _, _, total_rewards = td_agent.solve_early(maze, alpha)
        means[i] = total_rewards
        stds[i] = total_rewards

      means = np.mean(means, axis=0)
      stds = np.std(stds, axis=0)
      alphas_early_dict[alpha] = [means, stds]



    first_100_episodes = np.vstack(([alphas_early_dict[i][0][:100] for i in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]]))
    all_episodes = np.vstack([alphas_dict[i][0] for i in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]])
    first_100_episodes_mean = np.mean(first_100_episodes, axis=1)
    all_episodes_mean = np.mean(all_episodes, axis=1)
    x_axis = np.arange(0.1, 1.1, 0.1)


    plt.figure()
    plt.plot(x_axis, first_100_episodes_mean, "b")
    plt.plot(x_axis, all_episodes_mean, "r")
    plt.legend(["Interim Performance", "Asymptotic Performance"], loc="lower center")
    plt.title("Interim vs Asymptotic Performance varying alpha")
    plt.xlabel("alpha")
    plt.ylabel("sum of rewards per episode")
    plt.savefig("Interim_vs_Asymptotic.png")
    plt.show()

    class TD_agent(object):

      # [Action required]
      # WARNING: make sure this function can be called by the auto-marking script

      def create_episode(self, env, epsilon, Q, alpha, gamma):
        curr_step = env.reset()
        curr_state = curr_step[1]
        stop = curr_step[-1]
        total_reward = 0
        while not stop:
          probs = self.epsilon_greedy(env, curr_state, Q, epsilon)
          action = np.random.choice(range(env.get_action_size()), p=probs)
          next_step = env.step(action)
          stop = next_step[-1]
          curr_reward = next_step[2]
          total_reward += curr_reward
          next_state = next_step[1]
          self.update(Q, action, curr_state, next_state, curr_reward, alpha, gamma)
          curr_state = next_state

        return total_reward

      def convert_Q_to_policy(self,Q, policy):
        idxs = np.argmax(Q, axis=1)
        for state, idx in enumerate(idxs):
          policy[state][idx] = 1
        return policy




      def epsilon_greedy(self, env, curr_state, Q, epsilon):
        probs = [0] * env.get_action_size()
        optimal_action = np.argmax(Q[curr_state])
        for action in range(env.get_action_size()):
          if action == optimal_action:
            probs[action] = 1 - epsilon + (epsilon / env.get_action_size())
          else:
            probs[action]  = epsilon / env.get_action_size()

        return probs

      def update(self, Q, action, curr_state, next_state, curr_reward, alpha, gamma):
        optimal_next_Q = np.max(Q[next_state])
        curr_Q = Q[curr_state, action]
        #alpha = 1 / (iter)**2
        Q[curr_state, action] = curr_Q + alpha * (curr_reward + gamma * optimal_next_Q - curr_Q)



      def solve(self, env, epsilon):
        """
        Solve a given Maze environment using Temporal Difference learning
        input: env {Maze object} -- Maze to solve
        output:
          - policy {np.array} -- Optimal policy found to solve the given Maze environment
          - values {list of np.array} -- List of successive value functions for each episode
          - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
        """

        Q = np.random.rand(env.get_state_size(), env.get_action_size())
        V = np.zeros(env.get_state_size())
        policy = np.zeros((env.get_state_size(), env.get_action_size()))
        values = [V]
        total_rewards = []
        gamma = env.get_gamma()
        alpha = 0.05

        ####
        # Add your code here
        # WARNING: this agent only has access to env.reset() and env.step()
        # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value
        ####
        iter = 0
        while iter < 10000:

          total_reward = self.create_episode(env, epsilon, Q, alpha, gamma)
          new_V = np.max(Q, axis=1)
          values.append(new_V)

          total_rewards.append(total_reward)

          iter += 1

        policy = self.convert_Q_to_policy(Q, policy)

        return policy, values, total_rewards

    maze = Maze()
    epsilons_dict = {}

    epsilons = [0.2, 0.4, 0.6, 0.8]
    for epsilon in epsilons:



      means = np.zeros((5, 10000))
      stds = np.zeros((5, 10000))

      for i in range(5):
        td_agent = TD_agent()
        _, _, total_rewards = td_agent.solve(maze, epsilon)
        means[i] = total_rewards
        stds[i] = total_rewards

      means = np.mean(means, axis=0)
      stds = np.std(stds, axis=0)
      episodes = np.arange(1,10001)
      epsilons_dict[epsilon] = [means, stds]

      plt.figure()
      plt.plot(episodes, means, "b")
      plt.plot(episodes, stds, "r")
      plt.legend(["mean", "std"], loc="lower right")
      plt.title("mean vs std")
      plt.xlabel("number of episodes")
      plt.ylabel("discounted total reward")
      plt.savefig(f"learning_curve_of_epsilon={epsilon}.png")
      plt.show()



    epsilon_means = [epsilons_dict[i][0] for i in [0.2, 0.4, 0.6, 0.8]]
    colours = ['r','c','m','y']

    plt.figure()
    for i, epsilon_mean in enumerate(epsilon_means):
      plt.plot(episodes, epsilon_mean, alpha=0.7, color=colours[i])

    plt.legend([f"epsilon={0.2}", f"epsilon={0.4}", f"epsilon={0.6}", f"epsilon={0.8}"],loc="lower right")
    plt.title("mean of sum of total discounted total rewards per episode over 5 runs")
    plt.xlabel("number of episodes")
    plt.ylabel("total non-discounted sum of rewards")
    plt.savefig(f"epsilon.png")
    plt.show()

    """# Experiments - Question 1"""

    probs = [0.1, 0.25, 0.5]
    for prob in probs:

      print(f"prob_success: {prob}")
      class Maze(object):

        # [Action required]
        def __init__(self, prob):
          """
          Maze initialisation.
          input: /
          output: /
          """

          # [Action required]
          # Properties set from the CID
          self._prob_success = prob # float
          self._gamma = 0.8 + 0.02 * 6 # float
          self._goal = 0 # integer (0 for R0, 1 for R1, 2 for R2, 3 for R3)

          # Build the maze
          self._build_maze()


        # Functions used to build the Maze environment
        # You DO NOT NEED to modify them
        def _build_maze(self):
          """
          Maze initialisation.
          input: /
          output: /
          """

          # Properties of the maze
          self._shape = (13, 10)
          self._obstacle_locs = [
                                (1,0), (1,1), (1,2), (1,3), (1,4), (1,7), (1,8), (1,9), \
                                (2,1), (2,2), (2,3), (2,7), \
                                (3,1), (3,2), (3,3), (3,7), \
                                (4,1), (4,7), \
                                (5,1), (5,7), \
                                (6,5), (6,6), (6,7), \
                                (8,0), \
                                (9,0), (9,1), (9,2), (9,6), (9,7), (9,8), (9,9), \
                                (10,0)
                              ] # Location of obstacles
          self._absorbing_locs = [(2,0), (2,9), (10,1), (12,9)] # Location of absorbing states
          self._absorbing_rewards = [ (500 if (i == self._goal) else -50) for i in range (4) ]
          self._starting_locs = [(0,0), (0,1), (0,2), (0,3), (0,4), (0,5), (0,6), (0,7), (0,8), (0,9)] #Reward of absorbing states
          self._default_reward = -1 # Reward for each action performs in the environment
          self._max_t = 500 # Max number of steps in the environment

          # Actions
          self._action_size = 4
          self._direction_names = ['N','E','S','W'] # Direction 0 is 'N', 1 is 'E' and so on

          # States
          self._locations = []
          for i in range (self._shape[0]):
            for j in range (self._shape[1]):
              loc = (i,j)
              # Adding the state to locations if it is no obstacle
              if self._is_location(loc):
                self._locations.append(loc)
          self._state_size = len(self._locations)

          # Neighbours - each line is a state, ranked by state-number, each column is a direction (N, E, S, W)
          self._neighbours = np.zeros((self._state_size, 4))

          for state in range(self._state_size):
            loc = self._get_loc_from_state(state)

            # North
            neighbour = (loc[0]-1, loc[1]) # North neighbours location
            if self._is_location(neighbour):
              self._neighbours[state][self._direction_names.index('N')] = self._get_state_from_loc(neighbour)
            else: # If there is no neighbour in this direction, coming back to current state
              self._neighbours[state][self._direction_names.index('N')] = state

            # East
            neighbour = (loc[0], loc[1]+1) # East neighbours location
            if self._is_location(neighbour):
              self._neighbours[state][self._direction_names.index('E')] = self._get_state_from_loc(neighbour)
            else: # If there is no neighbour in this direction, coming back to current state
              self._neighbours[state][self._direction_names.index('E')] = state

            # South
            neighbour = (loc[0]+1, loc[1]) # South neighbours location
            if self._is_location(neighbour):
              self._neighbours[state][self._direction_names.index('S')] = self._get_state_from_loc(neighbour)
            else: # If there is no neighbour in this direction, coming back to current state
              self._neighbours[state][self._direction_names.index('S')] = state

            # West
            neighbour = (loc[0], loc[1]-1) # West neighbours location
            if self._is_location(neighbour):
              self._neighbours[state][self._direction_names.index('W')] = self._get_state_from_loc(neighbour)
            else: # If there is no neighbour in this direction, coming back to current state
              self._neighbours[state][self._direction_names.index('W')] = state

          # Absorbing
          self._absorbing = np.zeros((1, self._state_size))
          for a in self._absorbing_locs:
            absorbing_state = self._get_state_from_loc(a)
            self._absorbing[0, absorbing_state] = 1

          # Transition matrix
          self._T = np.zeros((self._state_size, self._state_size, self._action_size)) # Empty matrix of domension S*S*A
          for action in range(self._action_size):
            for outcome in range(4): # For each direction (N, E, S, W)
              # The agent has prob_success probability to go in the correct direction
              if action == outcome:
                prob = 1 - 3.0 * ((1.0 - self._prob_success) / 3.0) # (theoritically equal to self.prob_success but avoid rounding error and garanty a sum of 1)
              # Equal probability to go into one of the other directions
              else:
                prob = (1.0 - self._prob_success) / 3.0

              # Write this probability in the transition matrix
              for prior_state in range(self._state_size):
                # If absorbing state, probability of 0 to go to any other states
                if not self._absorbing[0, prior_state]:
                  post_state = self._neighbours[prior_state, outcome] # Post state number
                  post_state = int(post_state) # Transform in integer to avoid error
                  self._T[prior_state, post_state, action] += prob

          # Reward matrix
          self._R = np.ones((self._state_size, self._state_size, self._action_size)) # Matrix filled with 1
          self._R = self._default_reward * self._R # Set default_reward everywhere
          for i in range(len(self._absorbing_rewards)): # Set absorbing states rewards
            post_state = self._get_state_from_loc(self._absorbing_locs[i])
            self._R[:,post_state,:] = self._absorbing_rewards[i]

          # Creating the graphical Maze world
          self._graphics = GraphicsMaze(self._shape, self._locations, self._default_reward, self._obstacle_locs, self._absorbing_locs, self._absorbing_rewards, self._absorbing)

          # Reset the environment
          self.reset()


        def _is_location(self, loc):
          """
          Is the location a valid state (not out of Maze and not an obstacle)
          input: loc {tuple} -- location of the state
          output: _ {bool} -- is the location a valid state
          """
          if (loc[0] < 0 or loc[1] < 0 or loc[0] > self._shape[0]-1 or loc[1] > self._shape[1]-1):
            return False
          elif (loc in self._obstacle_locs):
            return False
          else:
            return True


        def _get_state_from_loc(self, loc):
          """
          Get the state number corresponding to a given location
          input: loc {tuple} -- location of the state
          output: index {int} -- corresponding state number
          """
          return self._locations.index(tuple(loc))


        def _get_loc_from_state(self, state):
          """
          Get the state number corresponding to a given location
          input: index {int} -- state number
          output: loc {tuple} -- corresponding location
          """
          return self._locations[state]

        # Getter functions used only for DP agents
        # You DO NOT NEED to modify them
        def get_T(self):
          return self._T

        def get_R(self):
          return self._R

        def get_absorbing(self):
          return self._absorbing

        # Getter functions used for DP, MC and TD agents
        # You DO NOT NEED to modify them
        def get_graphics(self):
          return self._graphics

        def get_action_size(self):
          return self._action_size

        def get_state_size(self):
          return self._state_size

        def get_gamma(self):
          return self._gamma

        # Functions used to perform episodes in the Maze environment
        def reset(self):
          """
          Reset the environment state to one of the possible starting states
          input: /
          output:
            - t {int} -- current timestep
            - state {int} -- current state of the envionment
            - reward {int} -- current reward
            - done {bool} -- True if reach a terminal state / 0 otherwise
          """
          self._t = 0
          self._state = self._get_state_from_loc(self._starting_locs[random.randrange(len(self._starting_locs))])
          self._reward = 0
          self._done = False
          return self._t, self._state, self._reward, self._done

        def step(self, action):
          """
          Perform an action in the environment
          input: action {int} -- action to perform
          output:
            - t {int} -- current timestep
            - state {int} -- current state of the envionment
            - reward {int} -- current reward
            - done {bool} -- True if reach a terminal state / 0 otherwise
          """

          # If environment already finished, print an error
          if self._done or self._absorbing[0, self._state]:
            print("Please reset the environment")
            return self._t, self._state, self._reward, self._done

          # Drawing a random number used for probaility of next state
          probability_success = random.uniform(0,1)

          # Look for the first possible next states (so get a reachable state even if probability_success = 0)
          new_state = 0
          while self._T[self._state, new_state, action] == 0:
            new_state += 1
          assert self._T[self._state, new_state, action] != 0, "Selected initial state should be probability 0, something might be wrong in the environment."

          # Find the first state for which probability of occurence matches the random value
          total_probability = self._T[self._state, new_state, action]
          while (total_probability < probability_success) and (new_state < self._state_size-1):
            new_state += 1
          total_probability += self._T[self._state, new_state, action]
          assert self._T[self._state, new_state, action] != 0, "Selected state should be probability 0, something might be wrong in the environment."

          # Setting new t, state, reward and done
          self._t += 1
          self._reward = self._R[self._state, new_state, action]
          self._done = self._absorbing[0, new_state] or self._t > self._max_t
          self._state = new_state
          return self._t, self._state, self._reward, self._done




      maze = Maze(prob)



      dp_agent = DP_agent()
      dp_policy, dp_value = dp_agent.solve(maze)

      print("Results of the DP agent:\n")
      maze.get_graphics().draw_policy(dp_policy)
      maze.get_graphics().draw_value(dp_value)
      print()

    gammas = [0.25, 0.5, 0.75]
    for gamma in gammas:

      print(f"gamma: {gamma}\n")
      class Maze(object):

        # [Action required]
        def __init__(self, gamma):
          """
          Maze initialisation.
          input: /
          output: /
          """

          # [Action required]
          # Properties set from the CID
          self._prob_success = 0.8 + 0.02 * (9-6)  # float
          self._gamma = gamma # float
          self._goal = 0 # integer (0 for R0, 1 for R1, 2 for R2, 3 for R3)

          # Build the maze
          self._build_maze()


        # Functions used to build the Maze environment
        # You DO NOT NEED to modify them
        def _build_maze(self):
          """
          Maze initialisation.
          input: /
          output: /
          """

          # Properties of the maze
          self._shape = (13, 10)
          self._obstacle_locs = [
                                (1,0), (1,1), (1,2), (1,3), (1,4), (1,7), (1,8), (1,9), \
                                (2,1), (2,2), (2,3), (2,7), \
                                (3,1), (3,2), (3,3), (3,7), \
                                (4,1), (4,7), \
                                (5,1), (5,7), \
                                (6,5), (6,6), (6,7), \
                                (8,0), \
                                (9,0), (9,1), (9,2), (9,6), (9,7), (9,8), (9,9), \
                                (10,0)
                              ] # Location of obstacles
          self._absorbing_locs = [(2,0), (2,9), (10,1), (12,9)] # Location of absorbing states
          self._absorbing_rewards = [ (500 if (i == self._goal) else -50) for i in range (4) ]
          self._starting_locs = [(0,0), (0,1), (0,2), (0,3), (0,4), (0,5), (0,6), (0,7), (0,8), (0,9)] #Reward of absorbing states
          self._default_reward = -1 # Reward for each action performs in the environment
          self._max_t = 500 # Max number of steps in the environment

          # Actions
          self._action_size = 4
          self._direction_names = ['N','E','S','W'] # Direction 0 is 'N', 1 is 'E' and so on

          # States
          self._locations = []
          for i in range (self._shape[0]):
            for j in range (self._shape[1]):
              loc = (i,j)
              # Adding the state to locations if it is no obstacle
              if self._is_location(loc):
                self._locations.append(loc)
          self._state_size = len(self._locations)

          # Neighbours - each line is a state, ranked by state-number, each column is a direction (N, E, S, W)
          self._neighbours = np.zeros((self._state_size, 4))

          for state in range(self._state_size):
            loc = self._get_loc_from_state(state)

            # North
            neighbour = (loc[0]-1, loc[1]) # North neighbours location
            if self._is_location(neighbour):
              self._neighbours[state][self._direction_names.index('N')] = self._get_state_from_loc(neighbour)
            else: # If there is no neighbour in this direction, coming back to current state
              self._neighbours[state][self._direction_names.index('N')] = state

            # East
            neighbour = (loc[0], loc[1]+1) # East neighbours location
            if self._is_location(neighbour):
              self._neighbours[state][self._direction_names.index('E')] = self._get_state_from_loc(neighbour)
            else: # If there is no neighbour in this direction, coming back to current state
              self._neighbours[state][self._direction_names.index('E')] = state

            # South
            neighbour = (loc[0]+1, loc[1]) # South neighbours location
            if self._is_location(neighbour):
              self._neighbours[state][self._direction_names.index('S')] = self._get_state_from_loc(neighbour)
            else: # If there is no neighbour in this direction, coming back to current state
              self._neighbours[state][self._direction_names.index('S')] = state

            # West
            neighbour = (loc[0], loc[1]-1) # West neighbours location
            if self._is_location(neighbour):
              self._neighbours[state][self._direction_names.index('W')] = self._get_state_from_loc(neighbour)
            else: # If there is no neighbour in this direction, coming back to current state
              self._neighbours[state][self._direction_names.index('W')] = state

          # Absorbing
          self._absorbing = np.zeros((1, self._state_size))
          for a in self._absorbing_locs:
            absorbing_state = self._get_state_from_loc(a)
            self._absorbing[0, absorbing_state] = 1

          # Transition matrix
          self._T = np.zeros((self._state_size, self._state_size, self._action_size)) # Empty matrix of domension S*S*A
          for action in range(self._action_size):
            for outcome in range(4): # For each direction (N, E, S, W)
              # The agent has prob_success probability to go in the correct direction
              if action == outcome:
                prob = 1 - 3.0 * ((1.0 - self._prob_success) / 3.0) # (theoritically equal to self.prob_success but avoid rounding error and garanty a sum of 1)
              # Equal probability to go into one of the other directions
              else:
                prob = (1.0 - self._prob_success) / 3.0

              # Write this probability in the transition matrix
              for prior_state in range(self._state_size):
                # If absorbing state, probability of 0 to go to any other states
                if not self._absorbing[0, prior_state]:
                  post_state = self._neighbours[prior_state, outcome] # Post state number
                  post_state = int(post_state) # Transform in integer to avoid error
                  self._T[prior_state, post_state, action] += prob

          # Reward matrix
          self._R = np.ones((self._state_size, self._state_size, self._action_size)) # Matrix filled with 1
          self._R = self._default_reward * self._R # Set default_reward everywhere
          for i in range(len(self._absorbing_rewards)): # Set absorbing states rewards
            post_state = self._get_state_from_loc(self._absorbing_locs[i])
            self._R[:,post_state,:] = self._absorbing_rewards[i]

          # Creating the graphical Maze world
          self._graphics = GraphicsMaze(self._shape, self._locations, self._default_reward, self._obstacle_locs, self._absorbing_locs, self._absorbing_rewards, self._absorbing)

          # Reset the environment
          self.reset()


        def _is_location(self, loc):
          """
          Is the location a valid state (not out of Maze and not an obstacle)
          input: loc {tuple} -- location of the state
          output: _ {bool} -- is the location a valid state
          """
          if (loc[0] < 0 or loc[1] < 0 or loc[0] > self._shape[0]-1 or loc[1] > self._shape[1]-1):
            return False
          elif (loc in self._obstacle_locs):
            return False
          else:
            return True


        def _get_state_from_loc(self, loc):
          """
          Get the state number corresponding to a given location
          input: loc {tuple} -- location of the state
          output: index {int} -- corresponding state number
          """
          return self._locations.index(tuple(loc))


        def _get_loc_from_state(self, state):
          """
          Get the state number corresponding to a given location
          input: index {int} -- state number
          output: loc {tuple} -- corresponding location
          """
          return self._locations[state]

        # Getter functions used only for DP agents
        # You DO NOT NEED to modify them
        def get_T(self):
          return self._T

        def get_R(self):
          return self._R

        def get_absorbing(self):
          return self._absorbing

        # Getter functions used for DP, MC and TD agents
        # You DO NOT NEED to modify them
        def get_graphics(self):
          return self._graphics

        def get_action_size(self):
          return self._action_size

        def get_state_size(self):
          return self._state_size

        def get_gamma(self):
          return self._gamma

        # Functions used to perform episodes in the Maze environment
        def reset(self):
          """
          Reset the environment state to one of the possible starting states
          input: /
          output:
            - t {int} -- current timestep
            - state {int} -- current state of the envionment
            - reward {int} -- current reward
            - done {bool} -- True if reach a terminal state / 0 otherwise
          """
          self._t = 0
          self._state = self._get_state_from_loc(self._starting_locs[random.randrange(len(self._starting_locs))])
          self._reward = 0
          self._done = False
          return self._t, self._state, self._reward, self._done

        def step(self, action):
          """
          Perform an action in the environment
          input: action {int} -- action to perform
          output:
            - t {int} -- current timestep
            - state {int} -- current state of the envionment
            - reward {int} -- current reward
            - done {bool} -- True if reach a terminal state / 0 otherwise
          """

          # If environment already finished, print an error
          if self._done or self._absorbing[0, self._state]:
            print("Please reset the environment")
            return self._t, self._state, self._reward, self._done

          # Drawing a random number used for probaility of next state
          probability_success = random.uniform(0,1)

          # Look for the first possible next states (so get a reachable state even if probability_success = 0)
          new_state = 0
          while self._T[self._state, new_state, action] == 0:
            new_state += 1
          assert self._T[self._state, new_state, action] != 0, "Selected initial state should be probability 0, something might be wrong in the environment."

          # Find the first state for which probability of occurence matches the random value
          total_probability = self._T[self._state, new_state, action]
          while (total_probability < probability_success) and (new_state < self._state_size-1):
            new_state += 1
          total_probability += self._T[self._state, new_state, action]
          assert self._T[self._state, new_state, action] != 0, "Selected state should be probability 0, something might be wrong in the environment."

          # Setting new t, state, reward and done
          self._t += 1
          self._reward = self._R[self._state, new_state, action]
          self._done = self._absorbing[0, new_state] or self._t > self._max_t
          self._state = new_state
          return self._t, self._state, self._reward, self._done




      maze = Maze(gamma)



      dp_agent = DP_agent()
      dp_policy, dp_value = dp_agent.solve(maze)

      print("Results of the DP agent:\n")
      maze.get_graphics().draw_policy(dp_policy)
      maze.get_graphics().draw_value(dp_value)
      print()

    if __name__ == "__main__":
    # This class define the Dynamic Programing agent

      class DP_agent(object):

        # [Action required]
        # WARNING: make sure this function can be called by the auto-marking script

        def solve(self, env):
          """
          Solve a given Maze environment using Dynamic Programming
          input: env {Maze object} -- Maze to solve
          output:
            - policy {np.array} -- Optimal policy found to solve the given Maze environment
            - V {np.array} -- Corresponding value function
          """

          # Initialisation (can be edited)
          policy = np.zeros((env.get_state_size(), env.get_action_size()))
          policy += 0.25
          V = np.zeros(env.get_state_size())
          theta = 0.0001
          gamma = env.get_gamma()
          delta = 2 * theta
          trans_matrix = env.get_T()
          rewards_matrix = env.get_R()

          return self.value_iteration(policy, V, theta, gamma, rewards_matrix, trans_matrix, env, delta)

        def value_iteration(self, policy, V, theta, gamma, rewards_matrix, trans_matrix, env, delta):
          while delta >= theta:
            delta = 0
            copied_V = V.copy()
            V_matrix = np.array([list(copied_V)] * 4)
            V_matrix = V_matrix.T

            for state in range(env.get_state_size()):
              v = V[state]
              reward = rewards_matrix[state]
              transition = trans_matrix[state]
              reward_value_sum = reward + gamma * V_matrix
              result_matrix = np.multiply(transition, reward_value_sum)
              V[state] = np.max(sum(result_matrix))
              delta = max(delta, abs(v - V[state]))

          copied_V = V.copy()
          V_matrix = np.array([list(copied_V)] * 4)
          V_matrix = V_matrix.T

          for state in range(env.get_state_size()):
            reward = rewards_matrix[state]
            transition = trans_matrix[state]
            reward_value_sum = reward + gamma * V_matrix
            result_matrix = np.multiply(transition, reward_value_sum)
            optimal_action = np.argmax(sum(result_matrix))
            policy[state] = np.zeros(policy.shape[1])
            policy[state][optimal_action] = 1

          return policy, V

      """

        def policy_evaluation(self, V, policy, theta, gamma, rewards_matrix, trans_matrix, env, delta):
          while delta >= theta:
            delta = 0
            copied_V = V.copy()
            V_matrix = np.array([list(copied_V)] * 4)
            V_matrix = V_matrix.T

            for state in range(env.get_state_size()):  # do we need to be updating the value function within the inner for loop?
              v = V[state]
              reward = rewards_matrix[state]
              transition = trans_matrix[state]
              policy_ = policy[state]
              reward_value_sum = reward + gamma * V_matrix
              policy_trans_prod = np.multiply(policy_, transition)
              result_matrix = np.multiply(policy_trans_prod, reward_value_sum)
              V[state] = sum(sum(result_matrix))
              delta = max(delta, abs(v - V[state]))



        def policy_improvement(self, V, gamma, policy, rewards_matrix, trans_matrix, env):
          policy_stable = True
          copied_policy = policy.copy()
          copied_V = V.copy()
          V_matrix = np.array([list(copied_V)] * 4)
          V_matrix = V_matrix.T

          for state in range(env.get_state_size()):
            b = copied_policy[state]
            reward = rewards_matrix[state]
            transition = trans_matrix[state]
            reward_value_sum = reward + gamma * V_matrix
            result_matrix = np.multiply(transition, reward_value_sum)
            result_array = sum(result_matrix)
            policy_idx = np.argmax(result_array)
            policy[state] = np.zeros(policy.shape[1])
            policy[state][policy_idx] = 1
            if (b != policy[state]).any():
              policy_stable = False
          return policy_stable

        def solve(self, env):
      """
      """
          Solve a given Maze environment using Dynamic Programming
          input: env {Maze object} -- Maze to solve
          output:
            - policy {np.array} -- Optimal policy found to solve the given Maze environment
            - V {np.array} -- Corresponding value function
          """
      """

          # Initialisation (can be edited)
          policy = np.zeros((env.get_state_size(), env.get_action_size()))
          policy += 0.25
          V = np.zeros(env.get_state_size())
          theta = 0.0001
          gamma = env.get_gamma()
          delta = 2 * theta
          trans_matrix = env.get_T()
          rewards_matrix = env.get_R()
          epochs = 0
          policy_stable = False

          ####
          # Add your code here
          # WARNING: for this agent only, you are allowed to access env.get_T(), env.get_R() and env.get_absorbing()
          ####
          while not policy_stable:
            self.policy_evaluation(V, policy, theta, gamma, rewards_matrix, trans_matrix, env, delta)
            policy_stable = self.policy_improvement(V, gamma, policy, rewards_matrix, trans_matrix, env)

            epochs += 1

          print(epochs)

          return policy, V




        def policy_evaluation_1(self, V, policy, theta, gamma, rewards_matrix, trans_matrix, env):


            copied_V = V.copy()
            V_matrix = np.array([list(copied_V)] * 4)
            V_matrix = V_matrix.T

            for state in range(env.get_state_size()):  # do we need to be updating the value function within the inner for loop?
              v = V[state]
              reward = rewards_matrix[state]
              transition = trans_matrix[state]
              policy_ = policy[state]
              reward_value_sum = reward + gamma * V_matrix
              policy_trans_prod = np.multiply(policy_, transition)
              result_matrix = np.multiply(policy_trans_prod, reward_value_sum)
              V[state] = sum(sum(result_matrix))

        def policy_evaluation_2(self, V, policy, theta, gamma, rewards_matrix, trans_matrix, env, delta):
          k = 0
          while delta >= theta and k < 2:
            delta = 0
            copied_V = V.copy()
            V_matrix = np.array([list(copied_V)] * 4)
            V_matrix = V_matrix.T

            for state in range(env.get_state_size()):  # do we need to be updating the value function within the inner for loop?
              v = V[state]
              reward = rewards_matrix[state]
              transition = trans_matrix[state]
              policy_ = policy[state]
              reward_value_sum = reward + gamma * V_matrix
              policy_trans_prod = np.multiply(policy_, transition)
              result_matrix = np.multiply(policy_trans_prod, reward_value_sum)
              V[state] = sum(sum(result_matrix))
              delta = max(delta, abs(v - V[state]))
            k += 1

        def policy_evaluation_3(self, V, policy, theta, gamma, rewards_matrix, trans_matrix, env, delta):
          k = 0
          while delta >= theta and k < 3:
            delta = 0
            copied_V = V.copy()
            V_matrix = np.array([list(copied_V)] * 4)
            V_matrix = V_matrix.T

            for state in range(env.get_state_size()):  # do we need to be updating the value function within the inner for loop?
              v = V[state]
              reward = rewards_matrix[state]
              transition = trans_matrix[state]
              policy_ = policy[state]
              reward_value_sum = reward + gamma * V_matrix
              policy_trans_prod = np.multiply(policy_, transition)
              result_matrix = np.multiply(policy_trans_prod, reward_value_sum)
              V[state] = sum(sum(result_matrix))
              delta = max(delta, abs(v - V[state]))
            k += 1

        def policy_evaluation_4(self, V, policy, theta, gamma, rewards_matrix, trans_matrix, env, delta):
          k = 0
          while delta >= theta and k < 4:
            delta = 0
            copied_V = V.copy()
            V_matrix = np.array([list(copied_V)] * 4)
            V_matrix = V_matrix.T

            for state in range(env.get_state_size()):  # do we need to be updating the value function within the inner for loop?
              v = V[state]
              reward = rewards_matrix[state]
              transition = trans_matrix[state]
              policy_ = policy[state]
              reward_value_sum = reward + gamma * V_matrix
              policy_trans_prod = np.multiply(policy_, transition)
              result_matrix = np.multiply(policy_trans_prod, reward_value_sum)
              V[state] = sum(sum(result_matrix))
              delta = max(delta, abs(v - V[state]))
            k += 1


        def policy_evaluation_5(self, V, policy, theta, gamma, rewards_matrix, trans_matrix, env, delta):
          k = 0
          while delta >= theta and k < 5:
            delta = 0
            copied_V = V.copy()
            V_matrix = np.array([list(copied_V)] * 4)
            V_matrix = V_matrix.T

            for state in range(env.get_state_size()):  # do we need to be updating the value function within the inner for loop?
              v = V[state]
              reward = rewards_matrix[state]
              transition = trans_matrix[state]
              policy_ = policy[state]
              reward_value_sum = reward + gamma * V_matrix
              policy_trans_prod = np.multiply(policy_, transition)
              result_matrix = np.multiply(policy_trans_prod, reward_value_sum)
              V[state] = sum(sum(result_matrix))
              delta = max(delta, abs(v - V[state]))
            k += 1

        def policy_evaluation_6(self, V, policy, theta, gamma, rewards_matrix, trans_matrix, env, delta):
          k = 0
          while delta >= theta and k < 6:
            delta = 0
            copied_V = V.copy()
            V_matrix = np.array([list(copied_V)] * 4)
            V_matrix = V_matrix.T

            for state in range(env.get_state_size()):  # do we need to be updating the value function within the inner for loop?
              v = V[state]
              reward = rewards_matrix[state]
              transition = trans_matrix[state]
              policy_ = policy[state]
              reward_value_sum = reward + gamma * V_matrix
              policy_trans_prod = np.multiply(policy_, transition)
              result_matrix = np.multiply(policy_trans_prod, reward_value_sum)
              V[state] = sum(sum(result_matrix))
              delta = max(delta, abs(v - V[state]))
            k += 1

        def policy_evaluation_7(self, V, policy, theta, gamma, rewards_matrix, trans_matrix, env, delta):
          k = 0
          while delta >= theta and k < 7:
            delta = 0
            copied_V = V.copy()
            V_matrix = np.array([list(copied_V)] * 4)
            V_matrix = V_matrix.T

            for state in range(env.get_state_size()):  # do we need to be updating the value function within the inner for loop?
              v = V[state]
              reward = rewards_matrix[state]
              transition = trans_matrix[state]
              policy_ = policy[state]
              reward_value_sum = reward + gamma * V_matrix
              policy_trans_prod = np.multiply(policy_, transition)
              result_matrix = np.multiply(policy_trans_prod, reward_value_sum)
              V[state] = sum(sum(result_matrix))
              delta = max(delta, abs(v - V[state]))
            k += 1

        def policy_evaluation_8(self, V, policy, theta, gamma, rewards_matrix, trans_matrix, env, delta):
          k = 0
          while delta >= theta and k < 8:
            delta = 0
            copied_V = V.copy()
            V_matrix = np.array([list(copied_V)] * 4)
            V_matrix = V_matrix.T

            for state in range(env.get_state_size()):  # do we need to be updating the value function within the inner for loop?
              v = V[state]
              reward = rewards_matrix[state]
              transition = trans_matrix[state]
              policy_ = policy[state]
              reward_value_sum = reward + gamma * V_matrix
              policy_trans_prod = np.multiply(policy_, transition)
              result_matrix = np.multiply(policy_trans_prod, reward_value_sum)
              V[state] = sum(sum(result_matrix))
              delta = max(delta, abs(v - V[state]))
            k += 1

        def policy_evaluation_9(self, V, policy, theta, gamma, rewards_matrix, trans_matrix, env, delta):
          k = 0
          while delta >= theta and k < 9:
            delta = 0
            copied_V = V.copy()
            V_matrix = np.array([list(copied_V)] * 4)
            V_matrix = V_matrix.T

            for state in range(env.get_state_size()):  # do we need to be updating the value function within the inner for loop?
              v = V[state]
              reward = rewards_matrix[state]
              transition = trans_matrix[state]
              policy_ = policy[state]
              reward_value_sum = reward + gamma * V_matrix
              policy_trans_prod = np.multiply(policy_, transition)
              result_matrix = np.multiply(policy_trans_prod, reward_value_sum)
              V[state] = sum(sum(result_matrix))
              delta = max(delta, abs(v - V[state]))
            k += 1

        def policy_evaluation_10(self, V, policy, theta, gamma, rewards_matrix, trans_matrix, env, delta):
          k = 0
          while delta >= theta and k < 10:
            delta = 0
            copied_V = V.copy()
            V_matrix = np.array([list(copied_V)] * 4)
            V_matrix = V_matrix.T

            for state in range(env.get_state_size()):  # do we need to be updating the value function within the inner for loop?
              v = V[state]
              reward = rewards_matrix[state]
              transition = trans_matrix[state]
              policy_ = policy[state]
              reward_value_sum = reward + gamma * V_matrix
              policy_trans_prod = np.multiply(policy_, transition)
              result_matrix = np.multiply(policy_trans_prod, reward_value_sum)
              V[state] = sum(sum(result_matrix))
              delta = max(delta, abs(v - V[state]))
            k += 1

        def solve_1(self, env):
          """
      """
          Solve a given Maze environment using Dynamic Programming
          input: env {Maze object} -- Maze to solve
          output:
            - policy {np.array} -- Optimal policy found to solve the given Maze environment
            - V {np.array} -- Corresponding value function
          """
      """

          # Initialisation (can be edited)
          policy = np.zeros((env.get_state_size(), env.get_action_size()))
          policy += 0.25
          V = np.zeros(env.get_state_size())
          theta = 0.0001
          gamma = env.get_gamma()
          delta = 2 * theta
          trans_matrix = env.get_T()
          rewards_matrix = env.get_R()
          epochs = 0
          policy_stable = False

          ####
          # Add your code here
          # WARNING: for this agent only, you are allowed to access env.get_T(), env.get_R() and env.get_absorbing()
          ####
          while not policy_stable:
            self.policy_evaluation_1(V, policy, theta, gamma, rewards_matrix, trans_matrix, env)
            policy_stable = self.policy_improvement(V, gamma, policy, rewards_matrix, trans_matrix, env)

            epochs += 1

          print(epochs)

          return policy, V

        def solve_2(self, env):
          """
      """
          Solve a given Maze environment using Dynamic Programming
          input: env {Maze object} -- Maze to solve
          output:
            - policy {np.array} -- Optimal policy found to solve the given Maze environment
            - V {np.array} -- Corresponding value function
          """
      """

          # Initialisation (can be edited)
          policy = np.zeros((env.get_state_size(), env.get_action_size()))
          policy += 0.25
          V = np.zeros(env.get_state_size())
          theta = 0.0001
          gamma = env.get_gamma()
          delta = 2 * theta
          trans_matrix = env.get_T()
          rewards_matrix = env.get_R()
          epochs = 0
          policy_stable = False

          ####
          # Add your code here
          # WARNING: for this agent only, you are allowed to access env.get_T(), env.get_R() and env.get_absorbing()
          ####
          while not policy_stable:
            self.policy_evaluation_2(V, policy, theta, gamma, rewards_matrix, trans_matrix, env, delta)
            policy_stable = self.policy_improvement(V, gamma, policy, rewards_matrix, trans_matrix, env)

            epochs += 1

          print(epochs)

          return policy, V

        def solve_3(self, env):
          """
      """
          Solve a given Maze environment using Dynamic Programming
          input: env {Maze object} -- Maze to solve
          output:
            - policy {np.array} -- Optimal policy found to solve the given Maze environment
            - V {np.array} -- Corresponding value function
          """
      """
          # Initialisation (can be edited)
          policy = np.zeros((env.get_state_size(), env.get_action_size()))
          policy += 0.25
          V = np.zeros(env.get_state_size())
          theta = 0.0001
          gamma = env.get_gamma()
          delta = 2 * theta
          trans_matrix = env.get_T()
          rewards_matrix = env.get_R()
          epochs = 0
          policy_stable = False

          ####
          # Add your code here
          # WARNING: for this agent only, you are allowed to access env.get_T(), env.get_R() and env.get_absorbing()
          ####
          while not policy_stable:
            self.policy_evaluation_3(V, policy, theta, gamma, rewards_matrix, trans_matrix, env, delta)
            policy_stable = self.policy_improvement(V, gamma, policy, rewards_matrix, trans_matrix, env)

            epochs += 1

          print(epochs)

          return policy, V

        def solve_4(self, env):
          """
      """
          Solve a given Maze environment using Dynamic Programming
          input: env {Maze object} -- Maze to solve
          output:
            - policy {np.array} -- Optimal policy found to solve the given Maze environment
            - V {np.array} -- Corresponding value function
          """
      """
          # Initialisation (can be edited)
          policy = np.zeros((env.get_state_size(), env.get_action_size()))
          policy += 0.25
          V = np.zeros(env.get_state_size())
          theta = 0.0001
          gamma = env.get_gamma()
          delta = 2 * theta
          trans_matrix = env.get_T()
          rewards_matrix = env.get_R()
          epochs = 0
          policy_stable = False

          ####
          # Add your code here
          # WARNING: for this agent only, you are allowed to access env.get_T(), env.get_R() and env.get_absorbing()
          ####
          while not policy_stable:
            self.policy_evaluation_4(V, policy, theta, gamma, rewards_matrix, trans_matrix, env, delta)
            policy_stable = self.policy_improvement(V, gamma, policy, rewards_matrix, trans_matrix, env)

            epochs += 1

          print(epochs)

          return policy, V

        def solve_5(self, env):
          """
      """
          Solve a given Maze environment using Dynamic Programming
          input: env {Maze object} -- Maze to solve
          output:
            - policy {np.array} -- Optimal policy found to solve the given Maze environment
            - V {np.array} -- Corresponding value function
          """
      """
          # Initialisation (can be edited)
          policy = np.zeros((env.get_state_size(), env.get_action_size()))
          policy += 0.25
          V = np.zeros(env.get_state_size())
          theta = 0.0001
          gamma = env.get_gamma()
          delta = 2 * theta
          trans_matrix = env.get_T()
          rewards_matrix = env.get_R()
          epochs = 0
          policy_stable = False

          ####
          # Add your code here
          # WARNING: for this agent only, you are allowed to access env.get_T(), env.get_R() and env.get_absorbing()
          ####
          while not policy_stable:
            self.policy_evaluation_5(V, policy, theta, gamma, rewards_matrix, trans_matrix, env, delta)
            policy_stable = self.policy_improvement(V, gamma, policy, rewards_matrix, trans_matrix, env)

            epochs += 1

          print(epochs)

          return policy, V

        def solve_6(self, env):
          """
      """
          Solve a given Maze environment using Dynamic Programming
          input: env {Maze object} -- Maze to solve
          output:
            - policy {np.array} -- Optimal policy found to solve the given Maze environment
            - V {np.array} -- Corresponding value function
          """
      """
          # Initialisation (can be edited)
          policy = np.zeros((env.get_state_size(), env.get_action_size()))
          policy += 0.25
          V = np.zeros(env.get_state_size())
          theta = 0.0001
          gamma = env.get_gamma()
          delta = 2 * theta
          trans_matrix = env.get_T()
          rewards_matrix = env.get_R()
          epochs = 0
          policy_stable = False

          ####
          # Add your code here
          # WARNING: for this agent only, you are allowed to access env.get_T(), env.get_R() and env.get_absorbing()
          ####
          while not policy_stable:
            self.policy_evaluation_6(V, policy, theta, gamma, rewards_matrix, trans_matrix, env, delta)
            policy_stable = self.policy_improvement(V, gamma, policy, rewards_matrix, trans_matrix, env)

            epochs += 1

          print(epochs)

          return policy, V

        def solve_7(self, env):
          """
      """
          Solve a given Maze environment using Dynamic Programming
          input: env {Maze object} -- Maze to solve
          output:
            - policy {np.array} -- Optimal policy found to solve the given Maze environment
            - V {np.array} -- Corresponding value function
          """
      """
          # Initialisation (can be edited)
          policy = np.zeros((env.get_state_size(), env.get_action_size()))
          policy += 0.25
          V = np.zeros(env.get_state_size())
          theta = 0.0001
          gamma = env.get_gamma()
          delta = 2 * theta
          trans_matrix = env.get_T()
          rewards_matrix = env.get_R()
          epochs = 0
          policy_stable = False

          ####
          # Add your code here
          # WARNING: for this agent only, you are allowed to access env.get_T(), env.get_R() and env.get_absorbing()
          ####
          while not policy_stable:
            self.policy_evaluation_7(V, policy, theta, gamma, rewards_matrix, trans_matrix, env, delta)
            policy_stable = self.policy_improvement(V, gamma, policy, rewards_matrix, trans_matrix, env)

            epochs += 1

          print(epochs)

          return policy, V

        def solve_8(self, env):
          """
      """
          Solve a given Maze environment using Dynamic Programming
          input: env {Maze object} -- Maze to solve
          output:
            - policy {np.array} -- Optimal policy found to solve the given Maze environment
            - V {np.array} -- Corresponding value function
          """
      """
          # Initialisation (can be edited)
          policy = np.zeros((env.get_state_size(), env.get_action_size()))
          policy += 0.25
          V = np.zeros(env.get_state_size())
          theta = 0.0001
          gamma = env.get_gamma()
          delta = 2 * theta
          trans_matrix = env.get_T()
          rewards_matrix = env.get_R()
          epochs = 0
          policy_stable = False

          ####
          # Add your code here
          # WARNING: for this agent only, you are allowed to access env.get_T(), env.get_R() and env.get_absorbing()
          ####
          while not policy_stable:
            self.policy_evaluation_8(V, policy, theta, gamma, rewards_matrix, trans_matrix, env, delta)
            policy_stable = self.policy_improvement(V, gamma, policy, rewards_matrix, trans_matrix, env)

            epochs += 1

          print(epochs)

          return policy, V

        def solve_9(self, env):
          """
      """
          Solve a given Maze environment using Dynamic Programming
          input: env {Maze object} -- Maze to solve
          output:
            - policy {np.array} -- Optimal policy found to solve the given Maze environment
            - V {np.array} -- Corresponding value function
          """
      """

          # Initialisation (can be edited)
          policy = np.zeros((env.get_state_size(), env.get_action_size()))
          policy += 0.25
          V = np.zeros(env.get_state_size())
          theta = 0.0001
          gamma = env.get_gamma()
          delta = 2 * theta
          trans_matrix = env.get_T()
          rewards_matrix = env.get_R()
          epochs = 0
          policy_stable = False

          ####
          # Add your code here
          # WARNING: for this agent only, you are allowed to access env.get_T(), env.get_R() and env.get_absorbing()
          ####
          while not policy_stable:
            self.policy_evaluation_1(V, policy, theta, gamma, rewards_matrix, trans_matrix, env)
            policy_stable = self.policy_improvement(V, gamma, policy, rewards_matrix, trans_matrix, env)

            epochs += 1
          epochs += 1
          self.policy_evaluation(V, policy, theta, gamma, rewards_matrix, trans_matrix, env, delta)

          print(epochs)

          return policy, V

        def solve_10(self, env):
          """
      """
          Solve a given Maze environment using Dynamic Programming
          input: env {Maze object} -- Maze to solve
          output:
            - policy {np.array} -- Optimal policy found to solve the given Maze environment
            - V {np.array} -- Corresponding value function
          """
      """

          # Initialisation (can be edited)
          policy = np.zeros((env.get_state_size(), env.get_action_size()))
          policy += 0.25
          V = np.zeros(env.get_state_size())
          theta = 0.0001
          gamma = env.get_gamma()
          delta = 2 * theta
          trans_matrix = env.get_T()
          rewards_matrix = env.get_R()
          epochs = 0
          policy_stable = False

          ####
          # Add your code here
          # WARNING: for this agent only, you are allowed to access env.get_T(), env.get_R() and env.get_absorbing()
          ####
          while not policy_stable:
            self.policy_evaluation_10(V, policy, theta, gamma, rewards_matrix, trans_matrix, env, delta)
            policy_stable = self.policy_improvement(V, gamma, policy, rewards_matrix, trans_matrix, env)

            epochs += 1

          print(epochs)

          return policy, V
      """

    if __name__ == "__main__":
      import time
      print("Creating the Maze:\n")
      maze = Maze()


      ### Question 1: Dynamic programming

      dp_agent = DP_agent()
      start = time.time()
      dp_policy, dp_value = dp_agent.solve_1(maze)
      end = time.time()
      print(f"When k = 1, it takes {end - start}\n")

      print("Results of the DP agent:\n")
      maze.get_graphics().draw_policy(dp_policy)
      maze.get_graphics().draw_value(dp_value)

      dp_agent = DP_agent()
      start = time.time()
      dp_policy, dp_value = dp_agent.solve_2(maze)
      end = time.time()
      print(f"When k = 2, it takes {end - start}\n")

      print("Results of the DP agent:\n")
      maze.get_graphics().draw_policy(dp_policy)
      maze.get_graphics().draw_value(dp_value)

      dp_agent = DP_agent()
      start = time.time()
      dp_policy, dp_value = dp_agent.solve_3(maze)
      end = time.time()
      print(f"When k = 3, it takes {end - start}\n")

      print("Results of the DP agent:\n")
      maze.get_graphics().draw_policy(dp_policy)
      maze.get_graphics().draw_value(dp_value)

      dp_agent = DP_agent()
      start = time.time()
      dp_policy, dp_value = dp_agent.solve_4(maze)
      end = time.time()
      print(f"When k = 4, it takes {end - start}\n")

      print("Results of the DP agent:\n")
      maze.get_graphics().draw_policy(dp_policy)
      maze.get_graphics().draw_value(dp_value)

      dp_agent = DP_agent()
      start = time.time()
      dp_policy, dp_value = dp_agent.solve_5(maze)
      end = time.time()
      print(f"When k = 5, it takes {end - start}\n")

      print("Results of the DP agent:\n")
      maze.get_graphics().draw_policy(dp_policy)
      maze.get_graphics().draw_value(dp_value)

      dp_agent = DP_agent()
      start = time.time()
      dp_policy, dp_value = dp_agent.solve_6(maze)
      end = time.time()
      print(f"When k = 6, it takes {end - start}\n")

      print("Results of the DP agent:\n")
      maze.get_graphics().draw_policy(dp_policy)
      maze.get_graphics().draw_value(dp_value)

      dp_agent = DP_agent()
      start = time.time()
      dp_policy, dp_value = dp_agent.solve_7(maze)
      end = time.time()
      print(f"When k = 7, it takes {end - start}\n")

      print("Results of the DP agent:\n")
      maze.get_graphics().draw_policy(dp_policy)
      maze.get_graphics().draw_value(dp_value)

      dp_agent = DP_agent()
      start = time.time()
      dp_policy, dp_value = dp_agent.solve_8(maze)
      end = time.time()
      print(f"When k = 8, it takes {end - start}\n")

      print("Results of the DP agent:\n")
      maze.get_graphics().draw_policy(dp_policy)
      maze.get_graphics().draw_value(dp_value)

      dp_agent = DP_agent()
      start = time.time()
      dp_policy, dp_value = dp_agent.solve_9(maze)
      end = time.time()
      print(f"When k = 9, it takes {end - start}\n")

      print("Results of the DP agent:\n")
      maze.get_graphics().draw_policy(dp_policy)
      maze.get_graphics().draw_value(dp_value)

      dp_agent = DP_agent()
      start = time.time()
      dp_policy, dp_value = dp_agent.solve_10(maze)
      end = time.time()
      print(f"When k = 10, it takes {end - start}\n")

      print("Results of the DP agent:\n")
      maze.get_graphics().draw_policy(dp_policy)
      maze.get_graphics().draw_value(dp_value)

      dp_agent = DP_agent()
      start = time.time()
      dp_policy, dp_value = dp_agent.solve(maze)
      end = time.time()
      print(f"When not k defined, it takes {end - start}\n")

      print("Results of the DP agent:\n")
      maze.get_graphics().draw_policy(dp_policy)
      maze.get_graphics().draw_value(dp_value)

      dp_agent = DP_agent()
      start = time.time()
      dp_policy, dp_value = dp_agent.solve_value(maze)
      end = time.time()
      print(f"Value iteration, it takes {end - start}\n")

      print("Results of the DP agent:\n")
      maze.get_graphics().draw_policy(dp_policy)
      maze.get_graphics().draw_value(dp_value)

    if __name__ == "__main__":
      import time
      print("Creating the Maze:\n")
      maze = Maze()

      dp_agent = DP_agent()
      start = time.time()
      dp_policy, dp_value = dp_agent.solve_5(maze)
      end = time.time()
      print(f"When k = 5, it takes {end - start}\n")

      print("Results of the DP agent:\n")
      maze.get_graphics().draw_policy(dp_policy)
      maze.get_graphics().draw_value(dp_value)

      dp_agent = DP_agent()
      start = time.time()
      dp_policy, dp_value = dp_agent.solve_6(maze)
      end = time.time()
      print(f"When k = 6, it takes {end - start}\n")

      print("Results of the DP agent:\n")
      maze.get_graphics().draw_policy(dp_policy)
      maze.get_graphics().draw_value(dp_value)

      dp_agent = DP_agent()
      start = time.time()
      dp_policy, dp_value = dp_agent.solve_7(maze)
      end = time.time()
      print(f"When k = 7, it takes {end - start}\n")

      print("Results of the DP agent:\n")
      maze.get_graphics().draw_policy(dp_policy)
      maze.get_graphics().draw_value(dp_value)

      dp_agent = DP_agent()
      start = time.time()
      dp_policy, dp_value = dp_agent.solve_9(maze)
      end = time.time()
      print(f"When k = 9, it takes {end - start}\n")

      print("Results of the DP agent:\n")
      maze.get_graphics().draw_policy(dp_policy)
      maze.get_graphics().draw_value(dp_value)

      dp_agent = DP_agent()
      start = time.time()
      dp_policy, dp_value = dp_agent.solve(maze)
      end = time.time()
      print(f"When not k defined, it takes {end - start}\n")

      print("Results of the DP agent:\n")
      maze.get_graphics().draw_policy(dp_policy)
      maze.get_graphics().draw_value(dp_value)

      dp_agent = DP_agent()
      start = time.time()
      dp_policy, dp_value = dp_agent.solve_value(maze)
      end = time.time()
      print(f"Value iteration, it takes {end - start}\n")

      print("Results of the DP agent:\n")
      maze.get_graphics().draw_policy(dp_policy)
      maze.get_graphics().draw_value(dp_value)

    if __name__ == "__main__":
      import time
      print("Creating the Maze:\n")
      maze = Maze()



      dp_agent = DP_agent()
      start = time.time()
      dp_policy, dp_value = dp_agent.solve_9(maze)
      end = time.time()
      print(f"When k = 9, it takes {end - start}\n")

      print("Results of the DP agent:\n")
      maze.get_graphics().draw_policy(dp_policy)
      maze.get_graphics().draw_value(dp_value)



      dp_agent = DP_agent()
      start = time.time()
      dp_policy, dp_value = dp_agent.solve_value(maze)
      end = time.time()
      print(f"Value iteration, it takes {end - start}\n")

      print("Results of the DP agent:\n")
      maze.get_graphics().draw_policy(dp_policy)
      maze.get_graphics().draw_value(dp_value)

    """# Experiments - Question 2"""

    print("Solve_1\n")
    mc_agent = MC_agent()
    mc_policy, mc_values, total_rewards = mc_agent.solve_1(maze)

    print("Results of the MC agent:\n")
    maze.get_graphics().draw_policy(mc_policy)
    maze.get_graphics().draw_value(mc_values[-1])

    print("Solve_2\n")
    mc_agent = MC_agent()
    mc_policy, mc_values, total_rewards = mc_agent.solve_2(maze)

    print("Results of the MC agent:\n")
    maze.get_graphics().draw_policy(mc_policy)
    maze.get_graphics().draw_value(mc_values[-1])

    print("Solve_3\n")
    mc_agent = MC_agent()
    mc_policy, mc_values, total_rewards = mc_agent.solve_3(maze)

    print("Results of the MC agent:\n")
    maze.get_graphics().draw_policy(mc_policy)
    maze.get_graphics().draw_value(mc_values[-1])

    print("Solve_4\n")
    mc_agent = MC_agent()
    mc_policy, mc_values, total_rewards = mc_agent.solve_4(maze)

    print("Results of the MC agent:\n")
    maze.get_graphics().draw_policy(mc_policy)
    maze.get_graphics().draw_value(mc_values[-1])

    print("Solve_5\n")
    mc_agent = MC_agent()
    mc_policy, mc_values, total_rewards = mc_agent.solve_5(maze)

    print("Results of the MC agent:\n")
    maze.get_graphics().draw_policy(mc_policy)
    maze.get_graphics().draw_value(mc_values[-1])

    maze = Maze()

    print("Solve_6\n")
    mc_agent = MC_agent()
    mc_policy, mc_values, total_rewards = mc_agent.solve_6(maze)

    print("Results of the MC agent:\n")
    maze.get_graphics().draw_policy(mc_policy)
    maze.get_graphics().draw_value(mc_values[-1])

    print("Solve_7\n")
    mc_agent = MC_agent()
    mc_policy, mc_values, total_rewards = mc_agent.solve_7(maze)

    print("Results of the MC agent:\n")
    maze.get_graphics().draw_policy(mc_policy)
    maze.get_graphics().draw_value(mc_values[-1])

    print("Solve_8\n")
    mc_agent = MC_agent()
    mc_policy, mc_values, total_rewards = mc_agent.solve_8(maze)

    print("Results of the MC agent:\n")
    maze.get_graphics().draw_policy(mc_policy)
    maze.get_graphics().draw_value(mc_values[-1])

    print("Solve_9\n")
    mc_agent = MC_agent()
    mc_policy, mc_values, total_rewards = mc_agent.solve_9(maze)

    print("Results of the MC agent:\n")
    maze.get_graphics().draw_policy(mc_policy)
    maze.get_graphics().draw_value(mc_values[-1])

    class MC_agent(object):

      # [Action required]
      def create_episode(self, env, policy):
        curr_step = env.reset()
        curr_state = curr_step[1]
        stop = curr_step[-1]
        episode = []
        while not stop:
          action = np.random.choice(range(env.get_action_size()), p=policy[curr_state])
          next_step = env.step(action)
          curr_reward = next_step[2]
          episode.append([(curr_state, action), curr_reward])
          curr_state = next_step[1]
          stop = next_step[-1]

        total_reward = sum([i[-1] for i in episode])

        return episode, total_reward

      def update(self, env, episode, gamma, returns, Q, policy, epsilon, values):
        reversed_episode = list(reversed(episode))
        G = 0
        for idx,step in enumerate(reversed_episode):
          G = gamma * G + step[-1]
          if step[0] not in [i[0] for i in reversed_episode[idx+1:]]:
            state = step[0][0]
            action = step[0][1]
            returns[(state, action)].append(G)
            Q[state, action] = np.mean(returns[(state, action)])
            optimal_action = np.argmax(Q[state])
            for action in range(env.get_action_size()):
              if action == optimal_action:
                policy[state, action] = 1 - epsilon + (epsilon / env.get_action_size())
              else:
                policy[state, action] = epsilon / env.get_action_size()

        new_V = np.sum(policy * Q, axis=1)
        values.append(new_V)







      # WARNING: make sure this function can be called by the auto-marking script
      def solve(self, env):
        """
        Solve a given Maze environment using Monte Carlo learning
        input: env {Maze object} -- Maze to solve
        output:
          - policy {np.array} -- Optimal policy found to solve the given Maze environment
          - values {list of np.array} -- List of successive value functions for each episode
          - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
        """

        # Initialisation (can be edited)
        Q = np.random.rand(env.get_state_size(), env.get_action_size())
        V = np.zeros(env.get_state_size())
        policy = np.zeros((env.get_state_size(), env.get_action_size()))
        policy += 0.25
        values = [V]
        total_rewards = []
        returns = {key : [] for key in [(state, action) for state in range(env.get_state_size()) for action in range(env.get_action_size())]}
        gamma = env.get_gamma()
        #k = 1
        epsilon = 0.5
        threshold = 0.50

        ####
        # Add your code here
        # WARNING: this agent only has access to env.reset() and env.step()
        # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value
        ####
        #stop = False
        #while not stop:
        iter = 0
        while iter < 7000:
          episode, reward = self.create_episode(env, policy)
          self.update(env, episode, gamma, returns, Q, policy, epsilon, values)
          total_rewards.append(reward)

          iter += 1

        return policy, values, total_rewards


      def update_01(self, env, episode, gamma, returns, Q, policy, epsilon, values):
        reversed_episode = list(reversed(episode))
        G = 0
        for idx,step in enumerate(reversed_episode):
          G = gamma * G + step[-1]
          if step[0] not in [i[0] for i in reversed_episode[idx+1:]]:
            state = step[0][0]
            action = step[0][1]
            if epsilon < 1:
              returns[(state, action)].append(G)
              Q[state, action] = np.mean(returns[(state, action)])
            else:
              Q[state, action] = Q[state, action] + 0.5 * (G - Q[state, action])
            optimal_action = np.argmax(Q[state])
            for action in range(env.get_action_size()):
              if action == optimal_action:
                policy[state, action] = 1 - epsilon + (epsilon / env.get_action_size())
              else:
                policy[state, action] = epsilon / env.get_action_size()

        new_V = np.sum(policy * Q, axis=1)
        values.append(new_V)

      def update_02(self, env, episode, gamma, returns, Q, policy, epsilon, values):
        reversed_episode = list(reversed(episode))
        G = 0
        for idx,step in enumerate(reversed_episode):
          G = gamma * G + step[-1]
          if step[0] not in [i[0] for i in reversed_episode[idx+1:]]:
            state = step[0][0]
            action = step[0][1]
            returns[(state, action)].append(G)
            #Q[state, action] = np.mean(returns[(state, action)])
            Q[state, action] = Q[state, action] + 0.005 * (returns[(state, action)][-1] - Q[state, action])
            optimal_action = np.argmax(Q[state])
            for action in range(env.get_action_size()):
              if action == optimal_action:
                policy[state, action] = 1 - epsilon + (epsilon / env.get_action_size())
              else:
                policy[state, action] = epsilon / env.get_action_size()

        new_V = np.sum(policy * Q, axis=1)
        values.append(new_V)

      def update_03(self, env, episode, gamma, returns, Q, policy, epsilon, values):
        reversed_episode = list(reversed(episode))
        G = 0
        for idx,step in enumerate(reversed_episode):
          G = gamma * G + step[-1]
          if step[0] not in [i[0] for i in reversed_episode[idx+1:]]:
            state = step[0][0]
            action = step[0][1]
            returns[(state, action)].append(G)
            #Q[state, action] = np.mean(returns[(state, action)])
            Q[state, action] = Q[state, action] + 0.001 * (returns[(state, action)][-1] - Q[state, action])
            optimal_action = np.argmax(Q[state])
            for action in range(env.get_action_size()):
              if action == optimal_action:
                policy[state, action] = 1 - epsilon + (epsilon / env.get_action_size())
              else:
                policy[state, action] = epsilon / env.get_action_size()

        new_V = np.sum(policy * Q, axis=1)
        values.append(new_V)

      def update_04(self, env, episode, gamma, returns, Q, policy, epsilon, values):
        reversed_episode = list(reversed(episode))
        G = 0
        for idx,step in enumerate(reversed_episode):
          G = gamma * G + step[-1]
          if step[0] not in [i[0] for i in reversed_episode[idx+1:]]:
            state = step[0][0]
            action = step[0][1]
            returns[(state, action)].append(G)
            #Q[state, action] = np.mean(returns[(state, action)])
            Q[state, action] = Q[state, action] + 0.01 * (returns[(state, action)][-1] - Q[state, action])
            optimal_action = np.argmax(Q[state])
            for action in range(env.get_action_size()):
              if action == optimal_action:
                policy[state, action] = 1 - epsilon + (epsilon / env.get_action_size())
              else:
                policy[state, action] = epsilon / env.get_action_size()

        new_V = np.sum(policy * Q, axis=1)
        values.append(new_V)

      def update_05(self, env, episode, gamma, returns, Q, policy, epsilon, values):
        reversed_episode = list(reversed(episode))
        G = 0
        for idx,step in enumerate(reversed_episode):
          G = gamma * G + step[-1]
          if step[0] not in [i[0] for i in reversed_episode[idx+1:]]:
            state = step[0][0]
            action = step[0][1]
            returns[(state, action)].append(G)
            #Q[state, action] = np.mean(returns[(state, action)])
            Q[state, action] = Q[state, action] + 0.5 * (returns[(state, action)][-1] - Q[state, action])
            optimal_action = np.argmax(Q[state])
            for action in range(env.get_action_size()):
              if action == optimal_action:
                policy[state, action] = 1 - epsilon + (epsilon / env.get_action_size())
              else:
                policy[state, action] = epsilon / env.get_action_size()

        new_V = np.sum(policy * Q, axis=1)
        values.append(new_V)

      def update_06(self, env, episode, gamma, returns, Q, policy, epsilon, values):
        reversed_episode = list(reversed(episode))
        G = 0
        for idx,step in enumerate(reversed_episode):
          G = gamma * G + step[-1]
          if step[0] not in [i[0] for i in reversed_episode[idx+1:]]:
            state = step[0][0]
            action = step[0][1]
            returns[(state, action)].append(G)
            #Q[state, action] = np.mean(returns[(state, action)])
            Q[state, action] = Q[state, action] + 0.6 * (returns[(state, action)][-1] - Q[state, action])
            optimal_action = np.argmax(Q[state])
            for action in range(env.get_action_size()):
              if action == optimal_action:
                policy[state, action] = 1 - epsilon + (epsilon / env.get_action_size())
              else:
                policy[state, action] = epsilon / env.get_action_size()

        new_V = np.sum(policy * Q, axis=1)
        values.append(new_V)

      def update_07(self, env, episode, gamma, returns, Q, policy, epsilon, values):
        reversed_episode = list(reversed(episode))
        G = 0
        for idx,step in enumerate(reversed_episode):
          G = gamma * G + step[-1]
          if step[0] not in [i[0] for i in reversed_episode[idx+1:]]:
            state = step[0][0]
            action = step[0][1]
            returns[(state, action)].append(G)
            #Q[state, action] = np.mean(returns[(state, action)])
            Q[state, action] = Q[state, action] + 0.7 * (returns[(state, action)][-1] - Q[state, action])
            optimal_action = np.argmax(Q[state])
            for action in range(env.get_action_size()):
              if action == optimal_action:
                policy[state, action] = 1 - epsilon + (epsilon / env.get_action_size())
              else:
                policy[state, action] = epsilon / env.get_action_size()

        new_V = np.sum(policy * Q, axis=1)
        values.append(new_V)

      def update_08(self, env, episode, gamma, returns, Q, policy, epsilon, values):
        reversed_episode = list(reversed(episode))
        G = 0
        for idx,step in enumerate(reversed_episode):
          G = gamma * G + step[-1]
          if step[0] not in [i[0] for i in reversed_episode[idx+1:]]:
            state = step[0][0]
            action = step[0][1]
            returns[(state, action)].append(G)
            #Q[state, action] = np.mean(returns[(state, action)])
            Q[state, action] = Q[state, action] + 0.8 * (returns[(state, action)][-1] - Q[state, action])
            optimal_action = np.argmax(Q[state])
            for action in range(env.get_action_size()):
              if action == optimal_action:
                policy[state, action] = 1 - epsilon + (epsilon / env.get_action_size())
              else:
                policy[state, action] = epsilon / env.get_action_size()

        new_V = np.sum(policy * Q, axis=1)
        values.append(new_V)

      def update_09(self, env, episode, gamma, returns, Q, policy, epsilon, values):
        reversed_episode = list(reversed(episode))
        G = 0
        for idx,step in enumerate(reversed_episode):
          G = gamma * G + step[-1]
          if step[0] not in [i[0] for i in reversed_episode[idx+1:]]:
            state = step[0][0]
            action = step[0][1]
            returns[(state, action)].append(G)
            #Q[state, action] = np.mean(returns[(state, action)])
            Q[state, action] = Q[state, action] + 0.05 * (returns[(state, action)][-1] - Q[state, action])
            optimal_action = np.argmax(Q[state])
            for action in range(env.get_action_size()):
              if action == optimal_action:
                policy[state, action] = 1 - epsilon + (epsilon / env.get_action_size())
              else:
                policy[state, action] = epsilon / env.get_action_size()

        new_V = np.sum(policy * Q, axis=1)
        values.append(new_V)

      def update_10(self, env, episode, gamma, returns, Q, policy, epsilon, values):
        reversed_episode = list(reversed(episode))
        G = 0
        for idx,step in enumerate(reversed_episode):
          G = gamma * G + step[-1]
          if step[0] not in [i[0] for i in reversed_episode[idx+1:]]:
            state = step[0][0]
            action = step[0][1]
            returns[(state, action)].append(G)
            #Q[state, action] = np.mean(returns[(state, action)])
            Q[state, action] = Q[state, action] + 1 * (returns[(state, action)][-1] - Q[state, action])
            optimal_action = np.argmax(Q[state])
            for action in range(env.get_action_size()):
              if action == optimal_action:
                policy[state, action] = 1 - epsilon + (epsilon / env.get_action_size())
              else:
                policy[state, action] = epsilon / env.get_action_size()

        new_V = np.sum(policy * Q, axis=1)
        values.append(new_V)

      def solve_01(self, env):
        """
        Solve a given Maze environment using Monte Carlo learning
        input: env {Maze object} -- Maze to solve
        output:
          - policy {np.array} -- Optimal policy found to solve the given Maze environment
          - values {list of np.array} -- List of successive value functions for each episode
          - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
        """

        # Initialisation (can be edited)
        Q = np.random.rand(env.get_state_size(), env.get_action_size())
        V = np.zeros(env.get_state_size())
        policy = np.zeros((env.get_state_size(), env.get_action_size()))
        policy += 0.25
        values = [V]
        total_rewards = []
        returns = {key : [] for key in [(state, action) for state in range(env.get_state_size()) for action in range(env.get_action_size())]}
        gamma = env.get_gamma()
        #k = 1
        epsilon = 1
        threshold = 0.50

        ####
        # Add your code here
        # WARNING: this agent only has access to env.reset() and env.step()
        # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value
        ####
        #stop = False
        #while not stop:
        iter = 0
        while iter < 10000:
          episode, reward = self.create_episode(env, policy)
          self.update_01(env, episode, gamma, returns, Q, policy, epsilon, values)
          total_rewards.append(reward)
          if iter > 100:
            if reward > 0:
              epsilon *= 0.999
          iter += 1

        return policy, values, total_rewards

      def solve_02(self, env):
        """
        Solve a given Maze environment using Monte Carlo learning
        input: env {Maze object} -- Maze to solve
        output:
          - policy {np.array} -- Optimal policy found to solve the given Maze environment
          - values {list of np.array} -- List of successive value functions for each episode
          - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
        """

        # Initialisation (can be edited)
        Q = np.random.rand(env.get_state_size(), env.get_action_size())
        V = np.zeros(env.get_state_size())
        policy = np.zeros((env.get_state_size(), env.get_action_size()))
        policy += 0.25
        values = [V]
        total_rewards = []
        returns = {key : [] for key in [(state, action) for state in range(env.get_state_size()) for action in range(env.get_action_size())]}
        gamma = env.get_gamma()
        #k = 1
        epsilon = 1
        threshold = 0.50

        ####
        # Add your code here
        # WARNING: this agent only has access to env.reset() and env.step()
        # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value
        ####
        #stop = False
        #while not stop:
        iter = 0
        while iter < 5000:
          episode, reward = self.create_episode(env, policy)
          self.update_02(env, episode, gamma, returns, Q, policy, epsilon, values)
          total_rewards.append(reward)
          if iter > 4000:
            epsilon *= 0.99
          elif iter > 3750:
            epsilon = 0.3
          elif iter > 3500:
            epsilon = 0.5
          elif iter > 3250:
            epsilon = 0.7
          elif iter > 3000:
            epsilon = 0.75
          elif iter > 2500:
            epsilon = 0.8
          elif iter > 2000:
            epsilon = 0.85
          elif iter > 1500:
            epsilon = 0.9
          elif iter > 1000:
            epsilon = 0.95
          iter += 1

        return policy, values, total_rewards

      def solve_03(self, env):
        """
        Solve a given Maze environment using Monte Carlo learning
        input: env {Maze object} -- Maze to solve
        output:
          - policy {np.array} -- Optimal policy found to solve the given Maze environment
          - values {list of np.array} -- List of successive value functions for each episode
          - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
        """

        # Initialisation (can be edited)
        Q = np.random.rand(env.get_state_size(), env.get_action_size())
        V = np.zeros(env.get_state_size())
        policy = np.zeros((env.get_state_size(), env.get_action_size()))
        policy += 0.25
        values = [V]
        total_rewards = []
        returns = {key : [] for key in [(state, action) for state in range(env.get_state_size()) for action in range(env.get_action_size())]}
        gamma = env.get_gamma()
        #k = 1
        epsilon = 1
        threshold = 0.50

        ####
        # Add your code here
        # WARNING: this agent only has access to env.reset() and env.step()
        # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value
        ####
        #stop = False
        #while not stop:
        iter = 0
        while iter < 5000:
          episode, reward = self.create_episode(env, policy)
          self.update_03(env, episode, gamma, returns, Q, policy, epsilon, values)
          total_rewards.append(reward)
          if iter > 4000:
            epsilon *= 0.99
          elif iter > 3750:
            epsilon = 0.3
          elif iter > 3500:
            epsilon = 0.5
          elif iter > 3250:
            epsilon = 0.7
          elif iter > 3000:
            epsilon = 0.75
          elif iter > 2500:
            epsilon = 0.8
          elif iter > 2000:
            epsilon = 0.85
          elif iter > 1500:
            epsilon = 0.9
          elif iter > 1000:
            epsilon = 0.95
          iter += 1

        return policy, values, total_rewards

      def solve_04(self, env):
        """
        Solve a given Maze environment using Monte Carlo learning
        input: env {Maze object} -- Maze to solve
        output:
          - policy {np.array} -- Optimal policy found to solve the given Maze environment
          - values {list of np.array} -- List of successive value functions for each episode
          - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
        """

        # Initialisation (can be edited)
        Q = np.random.rand(env.get_state_size(), env.get_action_size())
        V = np.zeros(env.get_state_size())
        policy = np.zeros((env.get_state_size(), env.get_action_size()))
        policy += 0.25
        values = [V]
        total_rewards = []
        returns = {key : [] for key in [(state, action) for state in range(env.get_state_size()) for action in range(env.get_action_size())]}
        gamma = env.get_gamma()
        #k = 1
        epsilon = 1
        threshold = 0.50

        ####
        # Add your code here
        # WARNING: this agent only has access to env.reset() and env.step()
        # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value
        ####
        #stop = False
        #while not stop:
        iter = 0
        while iter < 5000:
          episode, reward = self.create_episode(env, policy)
          self.update_04(env, episode, gamma, returns, Q, policy, epsilon, values)
          total_rewards.append(reward)
          if iter > 4000:
            epsilon *= 0.99
          elif iter > 3750:
            epsilon = 0.3
          elif iter > 3500:
            epsilon = 0.5
          elif iter > 3250:
            epsilon = 0.7
          elif iter > 3000:
            epsilon = 0.75
          elif iter > 2500:
            epsilon = 0.8
          elif iter > 2000:
            epsilon = 0.85
          elif iter > 1500:
            epsilon = 0.9
          elif iter > 1000:
            epsilon = 0.95
          iter += 1

        return policy, values, total_rewards

      def solve_05(self, env):
        """
        Solve a given Maze environment using Monte Carlo learning
        input: env {Maze object} -- Maze to solve
        output:
          - policy {np.array} -- Optimal policy found to solve the given Maze environment
          - values {list of np.array} -- List of successive value functions for each episode
          - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
        """

        # Initialisation (can be edited)
        Q = np.random.rand(env.get_state_size(), env.get_action_size())
        V = np.zeros(env.get_state_size())
        policy = np.zeros((env.get_state_size(), env.get_action_size()))
        policy += 0.25
        values = [V]
        total_rewards = []
        returns = {key : [] for key in [(state, action) for state in range(env.get_state_size()) for action in range(env.get_action_size())]}
        gamma = env.get_gamma()
        #k = 1
        epsilon = 1
        threshold = 0.50

        ####
        # Add your code here
        # WARNING: this agent only has access to env.reset() and env.step()
        # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value
        ####
        #stop = False
        #while not stop:
        iter = 0
        while iter < 5000:
          episode, reward = self.create_episode(env, policy)
          self.update_05(env, episode, gamma, returns, Q, policy, epsilon, values)
          total_rewards.append(reward)
          if iter > 4000:
            epsilon *= 0.99
          elif iter > 3750:
            epsilon = 0.3
          elif iter > 3500:
            epsilon = 0.5
          elif iter > 3250:
            epsilon = 0.7
          elif iter > 3000:
            epsilon = 0.75
          elif iter > 2500:
            epsilon = 0.8
          elif iter > 2000:
            epsilon = 0.85
          elif iter > 1500:
            epsilon = 0.9
          elif iter > 1000:
            epsilon = 0.95
          iter += 1

        return policy, values, total_rewards

      def solve_06(self, env):
        """
        Solve a given Maze environment using Monte Carlo learning
        input: env {Maze object} -- Maze to solve
        output:
          - policy {np.array} -- Optimal policy found to solve the given Maze environment
          - values {list of np.array} -- List of successive value functions for each episode
          - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
        """

        # Initialisation (can be edited)
        Q = np.random.rand(env.get_state_size(), env.get_action_size())
        V = np.zeros(env.get_state_size())
        policy = np.zeros((env.get_state_size(), env.get_action_size()))
        policy += 0.25
        values = [V]
        total_rewards = []
        returns = {key : [] for key in [(state, action) for state in range(env.get_state_size()) for action in range(env.get_action_size())]}
        gamma = env.get_gamma()
        #k = 1
        epsilon = 1
        threshold = 0.50

        ####
        # Add your code here
        # WARNING: this agent only has access to env.reset() and env.step()
        # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value
        ####
        #stop = False
        #while not stop:
        iter = 0
        while iter < 5000:
          episode, reward = self.create_episode(env, policy)
          self.update_06(env, episode, gamma, returns, Q, policy, epsilon, values)
          total_rewards.append(reward)
          if iter > 4000:
            epsilon *= 0.99
          elif iter > 3750:
            epsilon = 0.3
          elif iter > 3500:
            epsilon = 0.5
          elif iter > 3250:
            epsilon = 0.7
          elif iter > 3000:
            epsilon = 0.75
          elif iter > 2500:
            epsilon = 0.8
          elif iter > 2000:
            epsilon = 0.85
          elif iter > 1500:
            epsilon = 0.9
          elif iter > 1000:
            epsilon = 0.95
          iter += 1

        return policy, values, total_rewards

      def solve_07(self, env):
        """
        Solve a given Maze environment using Monte Carlo learning
        input: env {Maze object} -- Maze to solve
        output:
          - policy {np.array} -- Optimal policy found to solve the given Maze environment
          - values {list of np.array} -- List of successive value functions for each episode
          - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
        """

        # Initialisation (can be edited)
        Q = np.random.rand(env.get_state_size(), env.get_action_size())
        V = np.zeros(env.get_state_size())
        policy = np.zeros((env.get_state_size(), env.get_action_size()))
        policy += 0.25
        values = [V]
        total_rewards = []
        returns = {key : [] for key in [(state, action) for state in range(env.get_state_size()) for action in range(env.get_action_size())]}
        gamma = env.get_gamma()
        #k = 1
        epsilon = 1
        threshold = 0.50

        ####
        # Add your code here
        # WARNING: this agent only has access to env.reset() and env.step()
        # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value
        ####
        #stop = False
        #while not stop:
        iter = 0
        while iter < 5000:
          episode, reward = self.create_episode(env, policy)
          self.update_07(env, episode, gamma, returns, Q, policy, epsilon, values)
          total_rewards.append(reward)
          if iter > 4000:
            epsilon *= 0.99
          elif iter > 3750:
            epsilon = 0.3
          elif iter > 3500:
            epsilon = 0.5
          elif iter > 3250:
            epsilon = 0.7
          elif iter > 3000:
            epsilon = 0.75
          elif iter > 2500:
            epsilon = 0.8
          elif iter > 2000:
            epsilon = 0.85
          elif iter > 1500:
            epsilon = 0.9
          elif iter > 1000:
            epsilon = 0.95
          iter += 1

        return policy, values, total_rewards


      def solve_08(self, env):
        """
        Solve a given Maze environment using Monte Carlo learning
        input: env {Maze object} -- Maze to solve
        output:
          - policy {np.array} -- Optimal policy found to solve the given Maze environment
          - values {list of np.array} -- List of successive value functions for each episode
          - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
        """

        # Initialisation (can be edited)
        Q = np.random.rand(env.get_state_size(), env.get_action_size())
        V = np.zeros(env.get_state_size())
        policy = np.zeros((env.get_state_size(), env.get_action_size()))
        policy += 0.25
        values = [V]
        total_rewards = []
        returns = {key : [] for key in [(state, action) for state in range(env.get_state_size()) for action in range(env.get_action_size())]}
        gamma = env.get_gamma()
        #k = 1
        epsilon = 1
        threshold = 0.50

        ####
        # Add your code here
        # WARNING: this agent only has access to env.reset() and env.step()
        # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value
        ####
        #stop = False
        #while not stop:
        iter = 0
        while iter < 5000:
          episode, reward = self.create_episode(env, policy)
          self.update_08(env, episode, gamma, returns, Q, policy, epsilon, values)
          total_rewards.append(reward)
          if iter > 4000:
            epsilon *= 0.99
          elif iter > 3750:
            epsilon = 0.3
          elif iter > 3500:
            epsilon = 0.5
          elif iter > 3250:
            epsilon = 0.7
          elif iter > 3000:
            epsilon = 0.75
          elif iter > 2500:
            epsilon = 0.8
          elif iter > 2000:
            epsilon = 0.85
          elif iter > 1500:
            epsilon = 0.9
          elif iter > 1000:
            epsilon = 0.95
          iter += 1

        return policy, values, total_rewards

      def solve_09(self, env):
        """
        Solve a given Maze environment using Monte Carlo learning
        input: env {Maze object} -- Maze to solve
        output:
          - policy {np.array} -- Optimal policy found to solve the given Maze environment
          - values {list of np.array} -- List of successive value functions for each episode
          - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
        """

        # Initialisation (can be edited)
        Q = np.random.rand(env.get_state_size(), env.get_action_size())
        V = np.zeros(env.get_state_size())
        policy = np.zeros((env.get_state_size(), env.get_action_size()))
        policy += 0.25
        values = [V]
        total_rewards = []
        returns = {key : [] for key in [(state, action) for state in range(env.get_state_size()) for action in range(env.get_action_size())]}
        gamma = env.get_gamma()
        #k = 1
        epsilon = 0.4
        threshold = 0.50

        ####
        # Add your code here
        # WARNING: this agent only has access to env.reset() and env.step()
        # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value
        ####
        #stop = False
        #while not stop:
        iter = 0
        while iter < 7000:
          episode, reward = self.create_episode(env, policy)
          self.update_09(env, episode, gamma, returns, Q, policy, epsilon, values)
          total_rewards.append(reward)

          iter += 1

        return policy, values, total_rewards

      def solve_10(self, env):
        """
        Solve a given Maze environment using Monte Carlo learning
        input: env {Maze object} -- Maze to solve
        output:
          - policy {np.array} -- Optimal policy found to solve the given Maze environment
          - values {list of np.array} -- List of successive value functions for each episode
          - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
        """

        # Initialisation (can be edited)
        Q = np.random.rand(env.get_state_size(), env.get_action_size())
        V = np.zeros(env.get_state_size())
        policy = np.zeros((env.get_state_size(), env.get_action_size()))
        policy += 0.25
        values = [V]
        total_rewards = []
        returns = {key : [] for key in [(state, action) for state in range(env.get_state_size()) for action in range(env.get_action_size())]}
        gamma = env.get_gamma()
        #k = 1
        epsilon = 1
        threshold = 0.50

        ####
        # Add your code here
        # WARNING: this agent only has access to env.reset() and env.step()
        # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value
        ####
        #stop = False
        #while not stop:
        iter = 0
        while iter < 5000:
          episode, reward = self.create_episode(env, policy)
          self.update_10(env, episode, gamma, returns, Q, policy, epsilon, values)
          total_rewards.append(reward)
          if iter > 4000:
            epsilon *= 0.99
          elif iter > 3750:
            epsilon = 0.3
          elif iter > 3500:
            epsilon = 0.5
          elif iter > 3250:
            epsilon = 0.7
          elif iter > 3000:
            epsilon = 0.75
          elif iter > 2500:
            epsilon = 0.8
          elif iter > 2000:
            epsilon = 0.85
          elif iter > 1500:
            epsilon = 0.9
          elif iter > 1000:
            epsilon = 0.95
          iter += 1

        return policy, values, total_rewards

    maze = Maze()

    print("Solve_01\n")
    mc_agent = MC_agent()
    mc_policy, mc_values, total_rewards = mc_agent.solve_01(maze)

    print("Results of the MC agent:\n")
    maze.get_graphics().draw_policy(mc_policy)
    maze.get_graphics().draw_value(mc_values[-1])
    '''
    print("Solve_02\n")
    mc_agent = MC_agent()
    mc_policy, mc_values, total_rewards = mc_agent.solve_02(maze)

    print("Results of the MC agent:\n")
    maze.get_graphics().draw_policy(mc_policy)
    maze.get_graphics().draw_value(mc_values[-1])

    print("Solve_03\n")
    mc_agent = MC_agent()
    mc_policy, mc_values, total_rewards = mc_agent.solve_03(maze)

    print("Results of the MC agent:\n")
    maze.get_graphics().draw_policy(mc_policy)
    maze.get_graphics().draw_value(mc_values[-1])

    print("Solve_04\n")
    mc_agent = MC_agent()
    mc_policy, mc_values, total_rewards = mc_agent.solve_04(maze)

    print("Results of the MC agent:\n")
    maze.get_graphics().draw_policy(mc_policy)
    maze.get_graphics().draw_value(mc_values[-1])

    print("Solve_05\n")
    mc_agent = MC_agent()
    mc_policy, mc_values, total_rewards = mc_agent.solve_05(maze)

    print("Results of the MC agent:\n")
    maze.get_graphics().draw_policy(mc_policy)
    maze.get_graphics().draw_value(mc_values[-1])

    print("Solve_06\n")
    mc_agent = MC_agent()
    mc_policy, mc_values, total_rewards = mc_agent.solve_06(maze)

    print("Results of the MC agent:\n")
    maze.get_graphics().draw_policy(mc_policy)
    maze.get_graphics().draw_value(mc_values[-1])

    print("Solve_07\n")
    mc_agent = MC_agent()
    mc_policy, mc_values, total_rewards = mc_agent.solve_07(maze)

    print("Results of the MC agent:\n")
    maze.get_graphics().draw_policy(mc_policy)
    maze.get_graphics().draw_value(mc_values[-1])

    print("Solve_08\n")
    mc_agent = MC_agent()
    mc_policy, mc_values, total_rewards = mc_agent.solve_08(maze)

    print("Results of the MC agent:\n")
    maze.get_graphics().draw_policy(mc_policy)
    maze.get_graphics().draw_value(mc_values[-1])
    '''
    '''
    print("Solve_09\n")
    mc_agent = MC_agent()
    mc_policy, mc_values, total_rewards = mc_agent.solve_09(maze)

    print("Results of the MC agent:\n")
    maze.get_graphics().draw_policy(mc_policy)
    maze.get_graphics().draw_value(mc_values[-1])
    '''
    '''
    print("Solve_10\n")
    mc_agent = MC_agent()
    mc_policy, mc_values, total_rewards = mc_agent.solve_10(maze)

    print("Results of the MC agent:\n")
    maze.get_graphics().draw_policy(mc_policy)
    maze.get_graphics().draw_value(mc_values[-1])

    print("Solve\n")
    mc_agent = MC_agent()
    mc_policy, mc_values, total_rewards = mc_agent.solve(maze)

    print("Results of the MC agent:\n")
    maze.get_graphics().draw_policy(mc_policy)
    maze.get_graphics().draw_value(mc_values[-1])
    '''

    """# Experiments - Question 3"""

    class TD_agent(object):

      # [Action required]
      # WARNING: make sure this function can be called by the auto-marking script

      def create_episode(self, env, epsilon, Q, alpha, gamma):
        curr_step = env.reset()
        curr_state = curr_step[1]
        stop = curr_step[-1]
        total_reward = 0
        while not stop:
          probs = self.epsilon_greedy(env, curr_state, Q, epsilon)
          action = np.random.choice(range(env.get_action_size()), p=probs)
          epsilon *= 0.999
          next_step = env.step(action)
          stop = next_step[-1]
          curr_reward = next_step[2]
          total_reward += curr_reward
          next_state = next_step[1]
          self.update(Q, action, curr_state, next_state, curr_reward, alpha, gamma)
          curr_state = next_state

        return total_reward

      def create_episode_(self, env, epsilon, Q, alpha, gamma):
        curr_step = env.reset()
        curr_state = curr_step[1]
        stop = curr_step[-1]
        total_reward = 0
        while not stop:
          probs = self.epsilon_greedy(env, curr_state, Q, epsilon)
          action = np.random.choice(range(env.get_action_size()), p=probs)
          next_step = env.step(action)
          stop = next_step[-1]
          curr_reward = next_step[2]
          total_reward += curr_reward
          next_state = next_step[1]
          self.update(Q, action, curr_state, next_state, curr_reward, alpha, gamma)
          curr_state = next_state

        return total_reward

      def convert_Q_to_policy(self,Q, policy):
        idxs = np.argmax(Q, axis=1)
        for state, idx in enumerate(idxs):
          policy[state][idx] = 1
        return policy




      def epsilon_greedy(self, env, curr_state, Q, epsilon):
        probs = [0] * env.get_action_size()
        optimal_action = np.argmax(Q[curr_state])
        for action in range(env.get_action_size()):
          if action == optimal_action:
            probs[action] = 1 - epsilon + (epsilon / env.get_action_size())
          else:
            probs[action]  = epsilon / env.get_action_size()

        return probs

      def update(self, Q, action, curr_state, next_state, curr_reward, alpha, gamma):
        optimal_next_Q = np.max(Q[next_state])
        curr_Q = Q[curr_state, action]
        #alpha = 1 / (iter)**2
        Q[curr_state, action] = curr_Q + alpha * (curr_reward + gamma * optimal_next_Q - curr_Q)



      def solve_1(self, env):
        """
        Solve a given Maze environment using Temporal Difference learning
        input: env {Maze object} -- Maze to solve
        output:
          - policy {np.array} -- Optimal policy found to solve the given Maze environment
          - values {list of np.array} -- List of successive value functions for each episode
          - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
        """

        # Initialisation (can be edited)
        Q = np.random.rand(env.get_state_size(), env.get_action_size())
        V = np.zeros(env.get_state_size())
        policy = np.zeros((env.get_state_size(), env.get_action_size()))
        values = [V]
        total_rewards = []
        epsilon = 0.8
        gamma = env.get_gamma()
        alpha = 0.05

        ####
        # Add your code here
        # WARNING: this agent only has access to env.reset() and env.step()
        # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value
        ####
        iter = 0
        while iter < 10000:
          iter += 1

          total_reward = self.create_episode(env, epsilon, Q, alpha, gamma)
          new_V = np.max(Q, axis=1)
          values.append(new_V)

          total_rewards.append(total_reward)

        policy = self.convert_Q_to_policy(Q, policy)

        return policy, values, total_rewards

      def solve_2(self, env):
        """
        Solve a given Maze environment using Temporal Difference learning
        input: env {Maze object} -- Maze to solve
        output:
          - policy {np.array} -- Optimal policy found to solve the given Maze environment
          - values {list of np.array} -- List of successive value functions for each episode
          - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
        """

        # Initialisation (can be edited)
        Q = np.random.rand(env.get_state_size(), env.get_action_size())
        V = np.zeros(env.get_state_size())
        policy = np.zeros((env.get_state_size(), env.get_action_size()))
        values = [V]
        total_rewards = []
        epsilon = 1
        gamma = env.get_gamma()
        alpha = 0.9

        ####
        # Add your code here
        # WARNING: this agent only has access to env.reset() and env.step()
        # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value
        ####
        iter = 0
        while iter < 5000:
          iter += 1

          total_reward = self.create_episode(env, epsilon, Q, alpha, gamma)
          new_V = np.max(Q, axis=1)
          values.append(new_V)

          total_rewards.append(total_reward)

        policy = self.convert_Q_to_policy(Q, policy)

        return policy, values, total_rewards

      def solve_3(self, env):
        """
        Solve a given Maze environment using Temporal Difference learning
        input: env {Maze object} -- Maze to solve
        output:
          - policy {np.array} -- Optimal policy found to solve the given Maze environment
          - values {list of np.array} -- List of successive value functions for each episode
          - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
        """

        # Initialisation (can be edited)
        Q = np.random.rand(env.get_state_size(), env.get_action_size())
        V = np.zeros(env.get_state_size())
        policy = np.zeros((env.get_state_size(), env.get_action_size()))
        values = [V]
        total_rewards = []
        epsilon = 1
        gamma = env.get_gamma()
        alpha = 0.8

        ####
        # Add your code here
        # WARNING: this agent only has access to env.reset() and env.step()
        # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value
        ####
        iter = 0
        while iter < 5000:
          iter += 1

          total_reward = self.create_episode(env, epsilon, Q, alpha, gamma)
          new_V = np.max(Q, axis=1)
          values.append(new_V)

          total_rewards.append(total_reward)

        policy = self.convert_Q_to_policy(Q, policy)

        return policy, values, total_rewards

      def solve_4(self, env):
        """
        Solve a given Maze environment using Temporal Difference learning
        input: env {Maze object} -- Maze to solve
        output:
          - policy {np.array} -- Optimal policy found to solve the given Maze environment
          - values {list of np.array} -- List of successive value functions for each episode
          - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
        """

        # Initialisation (can be edited)
        Q = np.random.rand(env.get_state_size(), env.get_action_size())
        V = np.zeros(env.get_state_size())
        policy = np.zeros((env.get_state_size(), env.get_action_size()))
        values = [V]
        total_rewards = []
        epsilon = 1
        gamma = env.get_gamma()
        alpha = 0.7

        ####
        # Add your code here
        # WARNING: this agent only has access to env.reset() and env.step()
        # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value
        ####
        iter = 0
        while iter < 5000:
          iter += 1

          total_reward = self.create_episode(env, epsilon, Q, alpha, gamma)
          new_V = np.max(Q, axis=1)
          values.append(new_V)

          total_rewards.append(total_reward)

        policy = self.convert_Q_to_policy(Q, policy)

        return policy, values, total_rewards

      def solve_5(self, env):
        """
        Solve a given Maze environment using Temporal Difference learning
        input: env {Maze object} -- Maze to solve
        output:
          - policy {np.array} -- Optimal policy found to solve the given Maze environment
          - values {list of np.array} -- List of successive value functions for each episode
          - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
        """

        # Initialisation (can be edited)
        Q = np.random.rand(env.get_state_size(), env.get_action_size())
        V = np.zeros(env.get_state_size())
        policy = np.zeros((env.get_state_size(), env.get_action_size()))
        values = [V]
        total_rewards = []
        epsilon = 1
        gamma = env.get_gamma()
        alpha = 0.6

        ####
        # Add your code here
        # WARNING: this agent only has access to env.reset() and env.step()
        # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value
        ####
        iter = 0
        while iter < 5000:
          iter += 1

          total_reward = self.create_episode(env, epsilon, Q, alpha, gamma)
          new_V = np.max(Q, axis=1)
          values.append(new_V)

          total_rewards.append(total_reward)

        policy = self.convert_Q_to_policy(Q, policy)

        return policy, values, total_rewards

      def solve_6(self, env):
        """
        Solve a given Maze environment using Temporal Difference learning
        input: env {Maze object} -- Maze to solve
        output:
          - policy {np.array} -- Optimal policy found to solve the given Maze environment
          - values {list of np.array} -- List of successive value functions for each episode
          - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
        """

        # Initialisation (can be edited)
        Q = np.random.rand(env.get_state_size(), env.get_action_size())
        V = np.zeros(env.get_state_size())
        policy = np.zeros((env.get_state_size(), env.get_action_size()))
        values = [V]
        total_rewards = []
        epsilon = 1
        gamma = env.get_gamma()
        alpha = 0.5

        ####
        # Add your code here
        # WARNING: this agent only has access to env.reset() and env.step()
        # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value
        ####
        iter = 0
        while iter < 5000:
          iter += 1

          total_reward = self.create_episode(env, epsilon, Q, alpha, gamma)
          new_V = np.max(Q, axis=1)
          values.append(new_V)

          total_rewards.append(total_reward)

        policy = self.convert_Q_to_policy(Q, policy)

        return policy, values, total_rewards

      def solve_7(self, env):
        """
        Solve a given Maze environment using Temporal Difference learning
        input: env {Maze object} -- Maze to solve
        output:
          - policy {np.array} -- Optimal policy found to solve the given Maze environment
          - values {list of np.array} -- List of successive value functions for each episode
          - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
        """

        # Initialisation (can be edited)
        Q = np.random.rand(env.get_state_size(), env.get_action_size())
        V = np.zeros(env.get_state_size())
        policy = np.zeros((env.get_state_size(), env.get_action_size()))
        values = [V]
        total_rewards = []
        epsilon = 1
        gamma = env.get_gamma()
        alpha = 0.4

        ####
        # Add your code here
        # WARNING: this agent only has access to env.reset() and env.step()
        # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value
        ####
        iter = 0
        while iter < 5000:
          iter += 1

          total_reward = self.create_episode(env, epsilon, Q, alpha, gamma)
          new_V = np.max(Q, axis=1)
          values.append(new_V)

          total_rewards.append(total_reward)

        policy = self.convert_Q_to_policy(Q, policy)

        return policy, values, total_rewards
      def solve_8(self, env):
        """
        Solve a given Maze environment using Temporal Difference learning
        input: env {Maze object} -- Maze to solve
        output:
          - policy {np.array} -- Optimal policy found to solve the given Maze environment
          - values {list of np.array} -- List of successive value functions for each episode
          - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
        """

        # Initialisation (can be edited)
        Q = np.random.rand(env.get_state_size(), env.get_action_size())
        V = np.zeros(env.get_state_size())
        policy = np.zeros((env.get_state_size(), env.get_action_size()))
        values = [V]
        total_rewards = []
        epsilon = 1
        gamma = env.get_gamma()
        alpha = 0.3

        ####
        # Add your code here
        # WARNING: this agent only has access to env.reset() and env.step()
        # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value
        ####
        iter = 0
        while iter < 5000:
          iter += 1

          total_reward = self.create_episode(env, epsilon, Q, alpha, gamma)
          new_V = np.max(Q, axis=1)
          values.append(new_V)

          total_rewards.append(total_reward)

        policy = self.convert_Q_to_policy(Q, policy)

        return policy, values, total_rewards

      def solve_9(self, env):
        """
        Solve a given Maze environment using Temporal Difference learning
        input: env {Maze object} -- Maze to solve
        output:
          - policy {np.array} -- Optimal policy found to solve the given Maze environment
          - values {list of np.array} -- List of successive value functions for each episode
          - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
        """

        # Initialisation (can be edited)
        Q = np.random.rand(env.get_state_size(), env.get_action_size())
        V = np.zeros(env.get_state_size())
        policy = np.zeros((env.get_state_size(), env.get_action_size()))
        values = [V]
        total_rewards = []
        epsilon = 1
        gamma = env.get_gamma()
        alpha = 0.2

        ####
        # Add your code here
        # WARNING: this agent only has access to env.reset() and env.step()
        # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value
        ####
        iter = 0
        while iter < 5000:
          iter += 1

          total_reward = self.create_episode(env, epsilon, Q, alpha, gamma)
          new_V = np.max(Q, axis=1)
          values.append(new_V)

          total_rewards.append(total_reward)

        policy = self.convert_Q_to_policy(Q, policy)

        return policy, values, total_rewards

      def solve_10(self, env):
        """
        Solve a given Maze environment using Temporal Difference learning
        input: env {Maze object} -- Maze to solve
        output:
          - policy {np.array} -- Optimal policy found to solve the given Maze environment
          - values {list of np.array} -- List of successive value functions for each episode
          - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
        """

        # Initialisation (can be edited)
        Q = np.random.rand(env.get_state_size(), env.get_action_size())
        V = np.zeros(env.get_state_size())
        policy = np.zeros((env.get_state_size(), env.get_action_size()))
        values = [V]
        total_rewards = []
        epsilon = 1
        gamma = env.get_gamma()
        alpha = 0.1

        ####
        # Add your code here
        # WARNING: this agent only has access to env.reset() and env.step()
        # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value
        ####
        iter = 0
        while iter < 5000:
          iter += 1

          total_reward = self.create_episode(env, epsilon, Q, alpha, gamma)
          new_V = np.max(Q, axis=1)
          values.append(new_V)

          total_rewards.append(total_reward)

        policy = self.convert_Q_to_policy(Q, policy)

        return policy, values, total_rewards


      def solve_final_02(self, env):
        """
        Solve a given Maze environment using Temporal Difference learning
        input: env {Maze object} -- Maze to solve
        output:
          - policy {np.array} -- Optimal policy found to solve the given Maze environment
          - values {list of np.array} -- List of successive value functions for each episode
          - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
        """

        # Initialisation (can be edited)
        Q = np.random.rand(env.get_state_size(), env.get_action_size())
        V = np.zeros(env.get_state_size())
        policy = np.zeros((env.get_state_size(), env.get_action_size()))
        values = [V]
        total_rewards = []
        epsilon = 1
        gamma = env.get_gamma()
        alpha = 0.2

        ####
        # Add your code here
        # WARNING: this agent only has access to env.reset() and env.step()
        # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value
        ####
        iter = 0
        while iter < 10000:

          total_reward = self.create_episode_(env, epsilon, Q, alpha, gamma)
          new_V = np.max(Q, axis=1)
          values.append(new_V)

          total_rewards.append(total_reward)

          if iter > 4000:
            if total_reward > 0:
              epsilon *= 0.99
          elif iter > 3750:
            epsilon = 0.3
          elif iter > 3500:
            epsilon = 0.5
          elif iter > 3250:
            epsilon = 0.7
          elif iter > 3000:
            epsilon = 0.75
          elif iter > 2500:
            epsilon = 0.8
          elif iter > 2000:
            epsilon = 0.85
          elif iter > 1500:
            epsilon = 0.9
          elif iter > 1000:
            epsilon = 0.95
          iter += 1

        policy = self.convert_Q_to_policy(Q, policy)

        return policy, values, total_rewards

      def solve_final_01(self, env):
        """
        Solve a given Maze environment using Temporal Difference learning
        input: env {Maze object} -- Maze to solve
        output:
          - policy {np.array} -- Optimal policy found to solve the given Maze environment
          - values {list of np.array} -- List of successive value functions for each episode
          - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
        """

        # Initialisation (can be edited)
        Q = np.random.rand(env.get_state_size(), env.get_action_size())
        V = np.zeros(env.get_state_size())
        policy = np.zeros((env.get_state_size(), env.get_action_size()))
        values = [V]
        total_rewards = []
        epsilon = 1
        gamma = env.get_gamma()
        alpha = 0.1

        ####
        # Add your code here
        # WARNING: this agent only has access to env.reset() and env.step()
        # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value
        ####
        iter = 0
        while iter < 10000:

          total_reward = self.create_episode_(env, epsilon, Q, alpha, gamma)
          new_V = np.max(Q, axis=1)
          values.append(new_V)

          total_rewards.append(total_reward)

          if iter > 4000:
            if total_reward > 0:
              epsilon *= 0.99
          elif iter > 3750:
            epsilon = 0.3
          elif iter > 3500:
            epsilon = 0.5
          elif iter > 3250:
            epsilon = 0.7
          elif iter > 3000:
            epsilon = 0.75
          elif iter > 2500:
            epsilon = 0.8
          elif iter > 2000:
            epsilon = 0.85
          elif iter > 1500:
            epsilon = 0.9
          elif iter > 1000:
            epsilon = 0.95
          iter += 1

        policy = self.convert_Q_to_policy(Q, policy)

        return policy, values, total_rewards




      def solve_final_001(self, env):
        """
        Solve a given Maze environment using Temporal Difference learning
        input: env {Maze object} -- Maze to solve
        output:
          - policy {np.array} -- Optimal policy found to solve the given Maze environment
          - values {list of np.array} -- List of successive value functions for each episode
          - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
        """

        # Initialisation (can be edited)
        Q = np.random.rand(env.get_state_size(), env.get_action_size())
        V = np.zeros(env.get_state_size())
        policy = np.zeros((env.get_state_size(), env.get_action_size()))
        values = [V]
        total_rewards = []
        epsilon = 1
        gamma = env.get_gamma()
        alpha = 0.01

        ####
        # Add your code here
        # WARNING: this agent only has access to env.reset() and env.step()
        # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value
        ####
        iter = 0
        while iter < 10000:

          total_reward = self.create_episode_(env, epsilon, Q, alpha, gamma)
          new_V = np.max(Q, axis=1)
          values.append(new_V)

          total_rewards.append(total_reward)

          if iter > 4000:
            if total_reward > 0:
              epsilon *= 0.99
          elif iter > 3750:
            epsilon = 0.3
          elif iter > 3500:
            epsilon = 0.5
          elif iter > 3250:
            epsilon = 0.7
          elif iter > 3000:
            epsilon = 0.75
          elif iter > 2500:
            epsilon = 0.8
          elif iter > 2000:
            epsilon = 0.85
          elif iter > 1500:
            epsilon = 0.9
          elif iter > 1000:
            epsilon = 0.95
          iter += 1

        policy = self.convert_Q_to_policy(Q, policy)

        return policy, values, total_rewards

      def solve_final_0005(self, env):
        """
        Solve a given Maze environment using Temporal Difference learning
        input: env {Maze object} -- Maze to solve
        output:
          - policy {np.array} -- Optimal policy found to solve the given Maze environment
          - values {list of np.array} -- List of successive value functions for each episode
          - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
        """

        # Initialisation (can be edited)
        Q = np.random.rand(env.get_state_size(), env.get_action_size())
        V = np.zeros(env.get_state_size())
        policy = np.zeros((env.get_state_size(), env.get_action_size()))
        values = [V]
        total_rewards = []
        epsilon = 1
        gamma = env.get_gamma()
        alpha = 0.005

        ####
        # Add your code here
        # WARNING: this agent only has access to env.reset() and env.step()
        # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value
        ####
        iter = 0
        while iter < 10000:

          total_reward = self.create_episode_(env, epsilon, Q, alpha, gamma)
          new_V = np.max(Q, axis=1)
          values.append(new_V)

          total_rewards.append(total_reward)

          if iter > 4000:
            if total_reward > 0:
              epsilon *= 0.99
          elif iter > 3750:
            epsilon = 0.3
          elif iter > 3500:
            epsilon = 0.5
          elif iter > 3250:
            epsilon = 0.7
          elif iter > 3000:
            epsilon = 0.75
          elif iter > 2500:
            epsilon = 0.8
          elif iter > 2000:
            epsilon = 0.85
          elif iter > 1500:
            epsilon = 0.9
          elif iter > 1000:
            epsilon = 0.95
          iter += 1

        policy = self.convert_Q_to_policy(Q, policy)

        return policy, values, total_rewards

      def solve_final_0001(self, env):
        """
        Solve a given Maze environment using Temporal Difference learning
        input: env {Maze object} -- Maze to solve
        output:
          - policy {np.array} -- Optimal policy found to solve the given Maze environment
          - values {list of np.array} -- List of successive value functions for each episode
          - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
        """

        # Initialisation (can be edited)
        Q = np.random.rand(env.get_state_size(), env.get_action_size())
        V = np.zeros(env.get_state_size())
        policy = np.zeros((env.get_state_size(), env.get_action_size()))
        values = [V]
        total_rewards = []
        epsilon = 1
        gamma = env.get_gamma()
        alpha = 0.001

        ####
        # Add your code here
        # WARNING: this agent only has access to env.reset() and env.step()
        # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value
        ####
        iter = 0
        while iter < 10000:

          total_reward = self.create_episode_(env, epsilon, Q, alpha, gamma)
          new_V = np.max(Q, axis=1)
          values.append(new_V)

          total_rewards.append(total_reward)

          if iter > 4000:
            if total_reward > 0:
              epsilon *= 0.99
          elif iter > 3750:
            epsilon = 0.3
          elif iter > 3500:
            epsilon = 0.5
          elif iter > 3250:
            epsilon = 0.7
          elif iter > 3000:
            epsilon = 0.75
          elif iter > 2500:
            epsilon = 0.8
          elif iter > 2000:
            epsilon = 0.85
          elif iter > 1500:
            epsilon = 0.9
          elif iter > 1000:
            epsilon = 0.95
          iter += 1

        policy = self.convert_Q_to_policy(Q, policy)

        return policy, values, total_rewards

    maze = Maze()

    print("Solve_1\n")
    td_agent = TD_agent()
    td_policy, td_values, total_rewards = td_agent.solve_1(maze)

    print("Results of the TD agent:\n")
    maze.get_graphics().draw_policy(td_policy)
    maze.get_graphics().draw_value(td_values[-1])
    '''
    print("Solve_2\n")
    td_agent = TD_agent()
    td_policy, td_values, total_rewards = td_agent.solve_2(maze)

    print("Results of the TD agent:\n")
    maze.get_graphics().draw_policy(td_policy)
    maze.get_graphics().draw_value(td_values[-1])

    print("Solve_3\n")
    td_agent = TD_agent()
    td_policy, td_values, total_rewards = td_agent.solve_3(maze)

    print("Results of the TD agent:\n")
    maze.get_graphics().draw_policy(td_policy)
    maze.get_graphics().draw_value(td_values[-1])

    print("Solve_4\n")
    td_agent = TD_agent()
    td_policy, td_values, total_rewards = td_agent.solve_4(maze)

    print("Results of the TD agent:\n")
    maze.get_graphics().draw_policy(td_policy)
    maze.get_graphics().draw_value(td_values[-1])

    print("Solve_5\n")
    td_agent = TD_agent()
    td_policy, td_values, total_rewards = td_agent.solve_5(maze)

    print("Results of the TD agent:\n")
    maze.get_graphics().draw_policy(td_policy)
    maze.get_graphics().draw_value(td_values[-1])

    print("Solve_6\n")
    td_agent = TD_agent()
    td_policy, td_values, total_rewards = td_agent.solve_6(maze)

    print("Results of the TD agent:\n")
    maze.get_graphics().draw_policy(td_policy)
    maze.get_graphics().draw_value(td_values[-1])

    print("Solve_7\n")
    td_agent = TD_agent()
    td_policy, td_values, total_rewards = td_agent.solve_7(maze)

    print("Results of the TD agent:\n")
    maze.get_graphics().draw_policy(td_policy)
    maze.get_graphics().draw_value(td_values[-1])

    print("Solve_8\n")
    td_agent = TD_agent()
    td_policy, td_values, total_rewards = td_agent.solve_8(maze)

    print("Results of the TD agent:\n")
    maze.get_graphics().draw_policy(td_policy)
    maze.get_graphics().draw_value(td_values[-1])

    print("Solve_9\n")
    td_agent = TD_agent()
    td_policy, td_values, total_rewards = td_agent.solve_9(maze)

    print("Results of the TD agent:\n")
    maze.get_graphics().draw_policy(td_policy)
    maze.get_graphics().draw_value(td_values[-1])

    print("Solve_10\n")
    td_agent = TD_agent()
    td_policy, td_values, total_rewards = td_agent.solve_10(maze)

    print("Results of the TD agent:\n")
    maze.get_graphics().draw_policy(td_policy)
    maze.get_graphics().draw_value(td_values[-1])

    print("Solve_\n")
    td_agent = TD_agent()
    td_policy, td_values, total_rewards = td_agent.solve_(maze)

    print("Results of the TD agent:\n")
    maze.get_graphics().draw_policy(td_policy)
    maze.get_graphics().draw_value(td_values[-1])
    '''

    maze = Maze()

    print("Solve_final_02\n")
    td_agent = TD_agent()
    td_policy, td_values, total_rewards = td_agent.solve_final_02(maze)

    print("Results of the TD agent:\n")
    maze.get_graphics().draw_policy(td_policy)
    maze.get_graphics().draw_value(td_values[-1])

    print("Solve_final_01\n")
    td_agent = TD_agent()
    td_policy, td_values, total_rewards = td_agent.solve_final_01(maze)

    print("Results of the TD agent:\n")
    maze.get_graphics().draw_policy(td_policy)
    maze.get_graphics().draw_value(td_values[-1])

    print("Solve_final_005\n")
    td_agent = TD_agent()
    td_policy, td_values, total_rewards = td_agent.solve_final_005(maze)

    print("Results of the TD agent:\n")
    maze.get_graphics().draw_policy(td_policy)
    maze.get_graphics().draw_value(td_values[-1])

    print("Solve_final_001\n")
    td_agent = TD_agent()
    td_policy, td_values, total_rewards = td_agent.solve_final_001(maze)

    print("Results of the TD agent:\n")
    maze.get_graphics().draw_policy(td_policy)
    maze.get_graphics().draw_value(td_values[-1])

    print("Solve_final_0005\n")
    td_agent = TD_agent()
    td_policy, td_values, total_rewards = td_agent.solve_final_0005(maze)

    print("Results of the TD agent:\n")
    maze.get_graphics().draw_policy(td_policy)
    maze.get_graphics().draw_value(td_values[-1])

    print("Solve_final_0001\n")
    td_agent = TD_agent()
    td_policy, td_values, total_rewards = td_agent.solve_final_0001(maze)

    print("Results of the TD agent:\n")
    maze.get_graphics().draw_policy(td_policy)
    maze.get_graphics().draw_value(td_values[-1])


